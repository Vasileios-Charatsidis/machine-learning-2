%        File: hw1-sol.tex
%     Created: gio mar 31 12:00  2016 C
% Last Change: gio mar 31 12:00  2016 C
%
\documentclass{amsmlaj}

\begin{document}

\lecturesol{Homework 1}{Ke Tran}{m.k.tran@uva.nl}
{April 05, 2016}{Andrea Jemmett}{11162929}{andreajemmett@gmail.com}{N/A}


\begin{problem}
Consider two random vectors $\vt{x} \in \R^n$ and $\vt{z} \in \R^n$ having
Gaussian distribution  $p(\vt{x}) = \distNorm(\vt{x} | \vt{\mu}_{\vt{x}},
\vt{\Sigma}_{\vt{x}})$ and  $p(\vt{z}) = \distNorm(\vt{z} | \vt{\mu}_{\vt{z}},
\vt{\Sigma}_{\vt{z}})$.  Consider random vector $\vt{y} = \vt{x} + \vt{z}$.
Derive mean and covariance of $p(\vt{y})$.
\end{problem}

\begin{problem}
Given a set of $N$ observations $\mathcal{X} = \{x_1,\dots,x_N\}$.
Assume that $x_i \sim \distNorm(\mu,\sigma^2)$ where $\sigma^2$ is known
and $\mu \sim \distNorm(\mu_0,\sigma_0^2)$. 
\begin{enumerate}
\item Write down the likelihood of the data
				$p(\mathcal{X}|\mu,\sigma^2)$;
		
	\begin{equation}
		p(\mathcal{X}|\mu,\sigma^2)=\prod_{i=1}^{N}p(x_i|\mu,\sigma^2)
	\end{equation}

\item Write down the posterior $p(\mu|\mathcal{X},\sigma^2,
				\mu_0,\sigma_0^2)$;

	\begin{equation}
		\begin{split}
		p(\mu|\mathcal{X},\sigma^2,\mu_0,\sigma_0^2)
			&=p(\mathcal{X}|\mu,\sigma^2)p(\mu|\mu_0,\sigma_0^2) \\
			&=\prod_{i=1}^{N}p(x_i|\mu,\sigma^2)p(\mu|\mu_0,\sigma_0^2)
		\end{split}
	\end{equation}

\item Show that $p(\mu|\mathcal{X},\sigma^2, \mu_0,\sigma_0^2)$
				is a Gaussian distribution
				$\distNorm(\mu|\mu_N,\sigma_N^2)$ and find the values of
				$\mu_N$ and $\sigma_N^2$;

	\begin{equation}
		\begin{split}
		p(\mu|\mathcal{X},\sigma^2, \mu_0,\sigma_0^2)
		&=\prod_{i=1}^{N}\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left\{ -\frac{1}{2\sigma^2}(x_i-\mu)^2
		\right\} \frac{1}{\sqrt{2\pi\sigma_0^2}}\exp\left\{
		-\frac{1}{2\sigma_0^2}(\mu-\mu_0)^2 \right\} \\
		&=\frac{1}{2\pi\sqrt{\sigma^2\sigma_0^2}}\exp\left\{
			-\frac{1}{2\sigma^2}\sum_{i=1}^N (x_i-\mu)^2
			-\frac{1}{2\sigma_0^2}(\mu-\mu_0)^2
		\right\} \\
		&=\frac{1}{2\pi\sqrt{\sigma^2\sigma_0^2}}\exp\left\{
			-\frac{1}{2\sigma^2}\left(
			\sum_{i=1}^N x_i^2 - \sum_{i=1}^N 2x_i\mu + N\mu^2
			\right) -
			\frac{1}{2\sigma_0^2} (\mu^2 - 2\mu\mu_0 + \mu_0^2)
		\right\} \\
		&=\frac{1}{2\pi\sqrt{\sigma^2\sigma_0^2}}\exp\left\{ 
			-\frac{\mu^2}{2}\left( \frac{1}{\sigma_0^2}+\frac{N}{\sigma^2} \right)
			+
			\mu\left( 
			\frac{1}{\sigma^2}\sum_{i=1}^N x_i +
			\frac{\mu_0}{\sigma_0^2}
			\right) + \text{const}
		\right\} \\
		&=\frac{1}{2\pi\sqrt{\sigma^2\sigma_0^2}}\exp\left\{ 
			-\underbrace{\frac{\mu^2}{2}\left(
				\frac{1}{\sigma_0^2}+\frac{N}{\sigma^2}
			\right)}_{-\frac{1}{2\sigma_N^2}}
			+
			\mu\underbrace{\left( 
			\frac{N}{\sigma^2} \mu_\text{ML} +
			\frac{\mu_0}{\sigma_0^2}
			\right)}_{\frac{1}{\sigma_N^2}\mu_N} + \text{const}
		\right\}
		\end{split}
	\end{equation}
	where $\mu_\text{ML}=\frac{1}{N}\sum_{i=1}^N x_i$ is the sample mean, \textit{const} are terms
	not dependent on $\mu$ and it is a Gaussian
	distribution (because of the form of coefficients entering quadratic $\mu^2$
	and linear $\mu$ terms) with mean and variance given by
	\begin{equation}
		\begin{split}
			\frac{1}{\sigma_N^2}&=\frac{1}{\sigma_0^2}+\frac{N}{\sigma^2} \\
			\mu_N&=\left(
					\frac{N\mu_\text{ML}}{\sigma^2}+\frac{\mu_0}{\sigma_0^2}
				\right)\sigma_N^2 \\
				&=\left(
				\frac{N\mu_\text{ML}}{\sigma^2}+\frac{\mu_0}{\sigma_0^2}
				\right)\frac{\sigma_0^2\sigma^2}{\sigma^2+N\sigma_0^2} \\
				&=\frac{\sigma^2}{N\sigma_0^2+\sigma^2}\mu_0  +
					\frac{N\sigma_0^2}{N\sigma_0^2+\sigma^2}\mu_\text{ML}
		\end{split}
	\end{equation}

\item Derive the maximum a posterior solution for $\mu$;

	\begin{equation}
		\begin{split}
						\frac{\partial}{\partial\mu}\log p(\mu|\mathcal{X},\sigma^2,\mu_0,\sigma_0^2)
						&=\frac{\partial}{\partial\mu}\log\left(\frac{1}{2\pi\sqrt{\sigma^2\sigma_0^2}}\right)
						-\frac{\partial}{\partial\mu}\left[\frac{1}{2\sigma^2}\sum_{i=1}^N(x_i-\mu)^2
						\right]-\frac{\partial}{\partial\mu}\left[
										\frac{1}{2\sigma_0^2}(\mu-\mu_0)^2
						\right] \\
						&=\frac{N}{2\sigma^2}\frac{\partial}{\partial\mu}\mu^2+
						\frac{1}{2\sigma_0^2}\frac{\partial}{\partial\mu}\mu^2-
						\frac{1}{\sigma^2}\sum_{i=1}^N x_i\frac{\partial}{\partial\mu}\mu-
						\frac{\mu_0}{\sigma_0^2}\frac{\partial}{\partial\mu}\mu+
						\frac{\partial}{\partial\mu}\left[ 
										\frac{1}{2\sigma^2}\sum_{i=1}^N x_i^2 +
										\frac{1}{2\sigma_0^2}\mu_0^2
						\right] \\
						&=\frac{N}{\sigma^2}\mu +
						\frac{1}{\sigma_0^2}\mu -
						\frac{N\mu_\text{ML}}{\sigma^2} -
						\frac{\mu_0}{\sigma_0^2} \\
						&=
						\frac{N\sigma_0^2+\sigma^2}{\sigma^2\sigma_0^2}\mu -
						\frac{N\sigma_0^2\mu_\text{ML}+\mu_0\sigma^2}{\sigma^2\sigma_0^2}=0
		\end{split}
	\end{equation}
	we can then solve for $\mu$
	\begin{equation}
		\begin{split}
					\hat{\mu}_\text{MAP}&=
						\frac{N\sigma_0^2\mu_\text{ML}+\mu_0\sigma^2}{\sigma^2\sigma_0^2}
						\frac{\sigma^2\sigma_0^2}{N\sigma_0^2+\sigma^2} \\
						&=\frac{N\sigma_0^2\mu_\text{ML}+\sigma^2\mu_0}{N\sigma_0^2+\sigma^2} \\
						&=\frac{N\sigma_0^2}{N\sigma_0^2+\sigma^2}\mu_\text{ML}
						+\frac{\sigma^2}{N\sigma_0^2+\sigma^2}\mu_0 \\
						&=\mu_N
		\end{split}
	\end{equation}

\item Derive expressions for sequential update of $\mu_N$ and
				$\sigma_N^2$;

	First define $\mu_N^{(N)}$ as the estimated $\mu_N$ using $N$ samples.
	Then we can write:
	\begin{equation}
		\begin{split}
			\mu_N^{(N)}&=
			\frac{\sigma^2}{N\sigma_0^2+\sigma^2}\mu_0+
			\frac{N\sigma_0^2}{N\sigma_0^2+\sigma^2}\mu_\text{ML} \\
			&=\frac{\sigma^2}{N\sigma_0^2+\sigma^2}\mu_0+
			\frac{N\sigma_0^2}{N\sigma_0^2+\sigma^2}\frac{1}{N}\sum_{i=1}^N x_i \\
			&=\frac{\sigma^2}{N\sigma_0^2+\sigma^2}\mu_0+
			\frac{\sigma_0^2}{N\sigma_0^2+\sigma^2}\sum_{i=1}^N x_i \\
			&=\frac{\sigma^2}{N\sigma_0^2+\sigma^2}\mu_0+
			\frac{\sigma_0^2}{N\sigma_0^2+\sigma^2}\left(
				x_N + \sum_{i=1}^{N-1}x_i
			\right) \\
			&=\frac{\sigma^2}{N\sigma_0^2+\sigma^2}\mu_0+
			\frac{\sigma_0^2}{N\sigma_0^2+\sigma^2}\left[
				x_N + (N-1)\mu_\text{ML}^{(N-1)}
			\right] \\
			&=\frac{\sigma^2}{N\sigma_0^2+\sigma^2}\mu_0+
			\frac{(N-1)\sigma_0^2}{N\sigma_0^2+\sigma^2}\mu_\text{ML}^{(N-1)}+
			\frac{\sigma_0^2}{N\sigma_0^2+\sigma^2}x_N \\
			&=\frac{1}{N\sigma_0^2+\sigma^2}(\sigma^2\mu_0+(N-1)\sigma_0^2\mu_\text{ML}^{(N-1)})
			+\frac{\sigma_0^2}{N\sigma_0^2+\sigma^2}x_N \\
			&=\frac{(N-1)\sigma_0^2+\sigma^2}{N\sigma_0^2+\sigma^2}\left(
			\frac{\sigma^2}{(N-1)\sigma_0^2+\sigma^2}\mu_0+
			\frac{(N-1)\sigma_0^2}{(N-1)\sigma_0^2+\sigma^2}\mu_\text{ML}^{(N-1)}
			\right)+\frac{\sigma_0^2}{N\sigma_0^2+\sigma^2}x_N \\
			&=\frac{(N-1)\sigma_0^2+\sigma^2}{N\sigma_0^2+\sigma^2}\mu_N^{(N-1)}+
			\frac{\sigma_0^2}{N\sigma_0^2+\sigma^2}x_N
		\end{split}
	\end{equation}

\item Derive the same results (as in 5) starting from the
				posterior distribution $p(\mu|x_1,\dots,x_{N-1})$, and
				multiplying by the likelihood function $p(x_N |\mu) =
				\distNorm(x_N |\mu,\sigma^2)$.
\end{enumerate}
\end{problem}

\begin{problem}
Consider a $D$-dimensional Gaussian random variable $\vt{x}$ with
distribution $\distNorm(\vt{x}|\vt{\mu},\vt{\Sigma})$ in which the
covariance $\vt{\Sigma}$ is known and for which we wish to infer the
mean $\vt{\mu}$ from a set of observations
$\mathcal{X}=\{\vt{x}_1,\dotsc,\vt{x}_N\}$. 
\begin{enumerate}
\item Write down the likelihood of the data
				$p(\mathcal{X}|\vt{\mu},\vt{\Sigma})$;

	\begin{equation}
		\begin{split}
			p(\mathcal{X}|\vt{\mu},\vt{\Sigma})&=
			\prod_{i=1}^D p(\vt{x}_i|\vt{\mu},\vt{\Sigma}) \\
			&=\frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{|\Sigma|^\frac{1}{2}}\prod_{i=1}^D
			\exp\left\{ \frac{1}{2}(\vt{x}_i-\vt{\mu})^T\Sigma^{-1}(\vt{x}_i-\vt{\mu})
			\right\} \\
			&=\frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{|\Sigma|^\frac{1}{2}}\exp\left\{
				-\sum_{i=1}^D\frac{1}{2}(\vt{x}_i-\vt{\mu})^T\Sigma^{-1}(\vt{x}_i-\vt{\mu})
			\right\}
		\end{split}
	\end{equation}

\item Given a prior distribution
				$p(\vt{\mu})=\distNorm(\vt{\mu}|\vt{\mu}_0,\vt{\Sigma}_0)$,
				find the corresponding posterior distribution
				$p(\vt{\mu}|\mathcal{X},\vt{\Sigma},\vt{\mu}_0,\vt{\Sigma}_0)$.

	\begin{equation}
		\begin{split}
			p(\vt{\mu}|\mathcal{X},\Sigma,\vt{\mu}_0,\Sigma_0)&=
			p(\vt{\mu}|\vt{\mu}_0,\Sigma_0)p(\mathcal{X}|\vt{\mu},\Sigma,\vt{\mu}_0,\Sigma_0) \\
			&=\frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{|\Sigma_0|^{\frac{1}{2}}}\exp\left\{
				-\frac{1}{2}(\vt{\mu}-\vt{\mu}_0)^T\Sigma_0^{-1}(\vt{\mu}-\vt{\mu}_0)
			\right\} \\
			&\quad\frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{|\Sigma|^\frac{1}{2}}\exp\left\{
				-\sum_{i=1}^D\frac{1}{2}(\vt{x}_i-\vt{\mu})^T\Sigma^{-1}(\vt{x}_i-\vt{\mu})
			\right\} \\
			&=(2\pi)^{-D}|\Sigma\Sigma_0|^{-1}\exp\left\{
				-\frac{1}{2}(\vt{\mu}-\vt{\mu}_0)^T\Sigma_0^{-1}(\vt{\mu}-\vt{\mu}_0)
				-\sum_{i=1}^D\frac{1}{2}(\vt{x}_i-\vt{\mu})\Sigma^{-1}(\vt{x}_i-\vt{\mu})
			\right\}
		\end{split}
	\end{equation}

	We then observe that for a symmetric matrix $\vt{A}$, holds that
	\begin{equation}
		\vt{a}^T\vt{A}\vt{b}=\vt{b}^T\vt{A}\vt{a}
	\end{equation}
	and so
	\begin{equation} \label{eq:completing-squares}
		\begin{split}
			(\vt{a}-\vt{b})^T\vt{A}(\vt{a}-\vt{b})&=(\vt{a}^T-\vt{b}^T)\vt{A}(\vt{a}-\vt{b}) \\
			&=(\vt{a}^T-\vt{b}^T)(\vt{A}\vt{a}-\vt{A}\vt{b}) \\
			&=\vt{a}^T\vt{A}\vt{a}-\vt{a}^T\vt{A}\vt{b}-\vt{b}^T\vt{A}\vt{a}+\vt{b}^T\vt{A}\vt{b} \\
			&=\vt{a}^T\vt{A}\vt{a}+\vt{b}^T\vt{A}\vt{b}-\vt{a}^T\vt{A}\vt{b}-\vt{a}^T\vt{A}\vt{b} \\
			&=\vt{a}^T\vt{A}\vt{a}+\vt{b}^T\vt{A}\vt{b}-2\vt{a}^T\vt{A}\vt{b}
		\end{split}
	\end{equation}

	Because we know that both $\Sigma$ and $\Sigma_0$ are symmetric, we can write
	the exponential term of the posterior as

	\begin{multline} \label{}
			-\frac{1}{2}(\vt{\mu}-\vt{\mu}_0)^T\vt{\Sigma}_0^{-1}(\vt{\mu}-\vt{\mu}_0)
			-\sum_{i=1}^D\frac{1}{2}(\vt{x}_i-\vt{\mu})\vt{\Sigma}^{-1}(\vt{x}_i-\vt{\mu}) \\
			=-\frac{1}{2}\vt{\mu}^T\vt{\Sigma}_0^{-1}\vt{\mu}
			+\vt{\mu}^T\vt{\Sigma}_0^{-1}\vt{\mu}_0
			-\frac{1}{2}\vt{\mu}_0^T\vt{\Sigma}_0^{-1}\vt{\mu}_0
			-\frac{1}{2}\sum_{i=1}^D\vt{x}_i^T\vt{\Sigma}^{-1}\vt{x}_i
			+\sum_{i=1}^D\vt{x}_i^T\vt{\Sigma}^{-1}\vt{\mu}
			-\frac{D}{2}\vt{\mu}^T\vt{\Sigma}^{-1}\vt{\mu} \\
			=-\frac{1}{2}\vt{\mu}^T\vt{\Sigma}_0^{-1}\vt{\mu}
			-\frac{D}{2}\vt{\mu}^T\vt{\Sigma}^{-1}\vt{\mu}
			+\vt{\mu}^T\vt{\Sigma}_0^{-1}\vt{\mu}_0
			+\sum_{i=1}^D\vt{x}_i^T\vt{\Sigma}^{-1}\vt{\mu}
			-\frac{1}{2}\vt{\mu}_0^T\vt{\Sigma}_0^{-1}\vt{\mu}_0
			-\frac{1}{2}\sum_{i=1}^D\vt{x}_i^T\vt{\Sigma}^{-1}\vt{x}_i \\
			=-\frac{1}{2}\vt{\mu}^T\vt{\Sigma}_0^{-1}\vt{\mu}
			-\frac{D}{2}\vt{\mu}^T\vt{\Sigma}^{-1}\vt{\mu}
			+\vt{\mu}^T\vt{\Sigma}_0^{-1}\vt{\mu}_0
			+\vt{\mu}^T\vt{\Sigma}^{-1}\sum_{i=1}^D\vt{x}_i
			-\frac{1}{2}\vt{\mu}_0^T\vt{\Sigma}_0^{-1}\vt{\mu}_0
			-\frac{1}{2}\sum_{i=1}^D\vt{x}_i^T\vt{\Sigma}^{-1}\vt{x}_i \\
			=-\frac{1}{2}\vt{\mu}^T(\vt{\Sigma}_0^{-1}+D\vt{\Sigma}^{-1})\vt{\mu}
			+\vt{\mu}^T\left(
				\vt{\Sigma}_0^{-1}\vt{\mu}_0+\vt{\Sigma}^{-1}\sum_{i=1}^D\vt{x}_i
			\right)
			-\frac{1}{2}\vt{\mu}_0^T\vt{\Sigma}_0^{-1}\vt{\mu}_0
			-\frac{1}{2}\sum_{i=1}^D\vt{x}_i^T\vt{\Sigma}^{-1}\vt{x}_i
	\end{multline}

	We can finally write the posterior

	\begin{multline} \label{}
		p(\vt{\mu}|\mathcal{X},\Sigma,\vt{\mu}_0,\Sigma_0) = \\
		(2\pi)^{-D}|\Sigma\Sigma_0|^{-1}\exp\left\{
			-\frac{1}{2}\vt{\mu}^T(\vt{\Sigma}_0^{-1}+D\vt{\Sigma}^{-1})\vt{\mu}
				+\vt{\mu}^T\left(
				\vt{\Sigma}_0^{-1}\vt{\mu}_0+\vt{\Sigma}^{-1}\sum_{i=1}^D\vt{x}_i
			\right)
			-\frac{1}{2}\vt{\mu}_0^T\vt{\Sigma}_0^{-1}\vt{\mu}_0
			-\frac{1}{2}\sum_{i=1}^D\vt{x}_i^T\vt{\Sigma}^{-1}\vt{x}_i
		\right\}
	\end{multline}

\item Show that the posterior
				$p(\vt{\mu}|\mathcal{X},\vt{\Sigma},\vt{\mu}_0,\vt{\Sigma}_0)$
				is a Gaussian distribution with mean $\vt{\mu}_N$ and
				covariance $\vt{\Sigma}_N$

	\begin{align} \label{}
		p(\vt{\mu}|\mathcal{X},\Sigma,\vt{\mu}_0,\Sigma_0) &\propto
		\exp\left\{
			-\frac{1}{2}\vt{\mu}^T(\vt{\Sigma}_0^{-1}+D\vt{\Sigma}^{-1})\vt{\mu}
			+\vt{\mu}^T\left(
				\vt{\Sigma}_0^{-1}\vt{\mu}_0+\vt{\Sigma}^{-1}\sum_{i=1}^D\vt{x}_i
			\right)
		\right\} \\
		&=\exp\left\{
			-\frac{1}{2}\vt{\mu}^\trans\vt{\Sigma}_N^{-1}\vt{\mu}
			+\vt{\mu}^\trans\vt{\Sigma}_N^{-1}\vt{\mu}_N
		\right\} \\
		& \propto \distNorm(\vt{\mu}|\vt{\mu}_N,\vt{\Sigma}_N)
	\end{align}

\item Find $\vt{\mu}_N$ and $\vt{\Sigma}_N$

	\begin{equation}
		\begin{split}
			\vt{\Sigma}_N&=(\vt{\Sigma}_0^{-1}+D\vt{\Sigma}^{-1})^{-1} \\
			\vt{\mu}_N&=\vt{\Sigma}_N^{-1} \left(
				\vt{\Sigma}_0^{-1}\vt{\mu}_0+\vt{\Sigma}^{-1}\sum_{i=1}^D\vt{x}_i
			\right) \\
			&=\left(
				\vt{\Sigma}_0^{-1}+D\vt{\Sigma}^{-1}
			\right)\left(
			\vt{\Sigma}_0^{-1}\vt{\mu}_0+\vt{\Sigma}^{-1}\sum_{i=1}^D\vt{x}_i
			\right)
		\end{split}
	\end{equation}

\end{enumerate}
\end{problem}


\begin{problem}

\begin{enumerate}
\item Show that the product of two Gaussians gives another
				(un-normalized) Gaussian
				$$
				\distNorm(\vt{x}|\vt{a},\vt{A})\distNorm(\vt{x}|\vt{b},\vt{B})
				= K^{-1}\distNorm(\vt{x}|\vt{c},\vt{C})
				$$
				where $\vt{c}=\vt{C}(\vt{A}^{-1}\vt{a} +
				\vt{B}^{-1}\vt{b})$ and
				$\vt{C}=(\vt{A}^{-1}+\vt{B}^{-1})^{-1}$.

	\begin{equation} \label{eq:gaussian-product}
		\begin{split}
			\distNorm(\vt{x}|\vt{a},\vt{A})\distNorm(\vt{x}|\vt{b},\vt{B})&=
			\frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{|\vt{A}|^{\frac{1}{2}}}\exp\left\{
				-\frac{1}{2}(\vt{x}-\vt{a})^\trans\vt{A}^{-1}(\vt{x}-\vt{a})
			\right\} \\
			&\qquad\frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{|\vt{B}|^{\frac{1}{2}}}\exp\left\{
				-\frac{1}{2}(\vt{x}-\vt{b})^\trans\vt{B}^{-1}(\vt{x}-\vt{b})
			\right\} \\
			&=(2\pi)^{-D}|\vt{AB}|^{-\frac{1}{2}}\exp\underbrace{\left\{
							-\frac{1}{2}(\vt{x}-\vt{a})^\trans\vt{A}^{-1}(\vt{x}-\vt{a})
							-\frac{1}{2}(\vt{x}-\vt{b})^\trans\vt{B}^{-1}(\vt{x}-\vt{b})
			\right\}}_{f(\vt{x})}
		\end{split}
	\end{equation}

	We can then develop the exponential term using the results of
	\eqref{eq:completing-squares}

	\begin{equation}
		\begin{split}
			f(\vt{x})&=-\frac{1}{2}(\vt{x}-\vt{a})^\trans\vt{A}^{-1}(\vt{x}-\vt{a})
				-\frac{1}{2}(\vt{x}-\vt{b})^\trans\vt{B}^{-1}(\vt{x}-\vt{b}) \\
				&=-\frac{1}{2}\vt{x}^\trans\vt{A}^{-1}\vt{x}
				+\vt{x}^\trans\vt{A}^{-1}\vt{a}
				-\frac{1}{2}\vt{a}^\trans\vt{A}^{-1}\vt{a}
				-\frac{1}{2}\vt{x}^\trans\vt{B}^{-1}\vt{x}
				+\vt{x}^\trans\vt{B}^{-1}\vt{b}
				-\frac{1}{2}\vt{b}^\trans\vt{B}^{-1}\vt{b} \\
				&=-\frac{1}{2}\vt{x}^\trans(\vt{A}^{-1}+\vt{B}^{-1})\vt{x}
				+\vt{x}^\trans(\vt{A}^{-1}\vt{a}+\vt{B}^{-1}\vt{b})
				-\frac{1}{2}\vt{a}^\trans\vt{A}^{-1}\vt{a}
				-\frac{1}{2}\vt{b}^\trans\vt{B}^{-1}\vt{b} \\
				&=-\frac{1}{2}\vt{x}^\trans\vt{C}^{-1}\vt{x}
				+\vt{x}^\trans\vt{C}^{-1}\vt{c}
				-\frac{1}{2}\vt{a}^\trans\vt{A}^{-1}\vt{a}
				-\frac{1}{2}\vt{b}^\trans\vt{B}^{-1}\vt{b}
		\end{split}
	\end{equation}
	where the following substitution has been adopted
	\begin{equation}
		\begin{split}
						\vt{C}^{-1}&=\vt{A}^{-1}+\vt{B}^{-1} \\
						\vt{C}&=(\vt{A}^{-1}+\vt{B}^{-1})^{-1} \\
						& \\
						\vt{C}^{-1}\vt{c}&=\vt{A}^{-1}\vt{a}+\vt{B}^{-1}\vt{b} \\
						\vt{c}&=\vt{C}(\vt{A}^{-1}\vt{a}+\vt{B}^{-1}\vt{b}) \\
						&=(\vt{A}^{-1}+\vt{B}^{-1})^{-1}(\vt{A}^{-1}\vt{a}+\vt{B}^{-1}\vt{b})
		\end{split}
	\end{equation}

	Substituting back $f(\vt{x})$ into \eqref{eq:gaussian-product} we obtain
	\begin{equation} 
		\begin{split}
			\distNorm(\vt{x}|\vt{a},\vt{A})\distNorm(\vt{x}|\vt{b},\vt{B})
			&=\frac{1}{(2\pi)^D}\frac{1}{|\vt{AB}|^{\frac{1}{2}}}\exp\{f(\vt{x})\} \\
			&=\frac{1}{(2\pi)^D}\frac{1}{|\vt{AB}|^{\frac{1}{2}}}\exp\left\{
							f(\vt{x})-\frac{1}{2}\vt{c}^\trans\vt{C}^{-1}\vt{c}
							+\frac{1}{2}\vt{c}^\trans\vt{C}^{-1}\vt{c}
			\right\} \\
			&=\frac{1}{(2\pi)^D}\frac{1}{|\vt{AB}|^{\frac{1}{2}}}\exp\left\{
							-\frac{1}{2}\vt{x}^\trans\vt{C}^{-1}\vt{x}
							+\vt{x}^\trans\vt{C}^{-1}\vt{c}
							-\frac{1}{2}\vt{c}^\trans\vt{C}^{-1}\vt{c} \right.\nonumber \\
							&\qquad \left.+\frac{1}{2}\vt{c}^\trans\vt{C}^{-1}\vt{c}
							-\frac{1}{2}\vt{a}^\trans\vt{A}^{-1}\vt{a}
							-\frac{1}{2}\vt{b}^\trans\vt{B}^{-1}\vt{b}
			\right\} \\
			&=\frac{1}{(2\pi)^D}\frac{1}{|\vt{AB}|^{\frac{1}{2}}}\exp\left\{
							-\frac{1}{2}(\vt{x}-\vt{c})^\trans\vt{C}^{-1}(\vt{x}-\vt{c})
							+\frac{1}{2}\vt{c}^\trans\vt{C}^{-1}\vt{c}
							-\frac{1}{2}\vt{a}^\trans\vt{A}^{-1}\vt{a}
							-\frac{1}{2}\vt{b}^\trans\vt{B}^{-1}\vt{b}
			\right\} \\
			&=\frac{1}{(2\pi)^{\frac{D}{2}}}\frac{|\vt{C}|^{\frac{1}{2}}}{|\vt{AB}|^{\frac{1}{2}}}\exp\left\{
							+\frac{1}{2}\vt{c}^\trans\vt{C}^{-1}\vt{c}
							-\frac{1}{2}\vt{a}^\trans\vt{A}^{-1}\vt{a}
							-\frac{1}{2}\vt{b}^\trans\vt{B}^{-1}\vt{b}
			\right\} \\
			&\qquad \frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{|\vt{C}|^{\frac{1}{2}}}\exp\left\{
							-\frac{1}{2}(\vt{x}-\vt{c})^\trans\vt{C}^{-1}(\vt{x}-\vt{c})
			\right\} \\
			&=K^{-1}\frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{|\vt{C}|^{\frac{1}{2}}}\exp\left\{
							-\frac{1}{2}(\vt{x}-\vt{c})^\trans\vt{C}^{-1}(\vt{x}-\vt{c})
			\right\} \\
			&=K^{-1}\distNorm(\vt{x}|\vt{c},\vt{C})
		\end{split}
	\end{equation}

\item Using the \emph{matrix inversion lemma}, also known as the
				the Woodbury, Sherman \& Morrison formula: 
				\begin{equation}
					(\vt{Z}+\vt{U}\vt{W}\vt{V}^\trans)^{-1} =
					\vt{Z}^{-1} -
					\vt{Z}^{-1}\vt{U}(\vt{W}^{-1}+\vt{V}^\trans
					\vt{Z}^{-1}\vt{U})^{-1}\vt{V}^\trans \vt{Z}^{-1}
				\end{equation}
				Proof that $\vt{C} =
				(\vt{A}^{-1}+\vt{B}^{-1})^{-1}=\vt{A} -
				\vt{A}(\vt{A}+\vt{B})^{-1}\vt{A} = \vt{B} -
				\vt{B}(\vt{A}+\vt{B})^{-1}\vt{B}$

	\begin{equation}
		\begin{split}
			\vt{C}&=(\vt{A}^{-1}+\vt{B}^{-1})^{-1}
			=(\vt{Z}+\vt{U}\vt{W}\vt{V}^\trans)
			=(\vt{A}^{-1}+\vt{I}\vt{B}^{-1}\vt{I})^{-1} \\
			&=(\vt{A}^{-1})^{-1}-(\vt{A}^{-1})^{-1}[(\vt{B}^{-1})^{-1}
			+\vt{I}(\vt{A}^{-1})^{-1}\vt{I}]^{-1}(\vt{A}^{-1})^{-1} \\
			&=\vt{A}-\vt{A}(\vt{B}+\vt{I}\vt{A}\vt{I})^{-1}\vt{I}\vt{A} \\
			&=\vt{A}-\vt{A}(\vt{B}+\vt{A})^{-1}\vt{A} \\
			&=\vt{A}-\vt{A}(\vt{A}+\vt{B})^{-1}\vt{A}
		\end{split}
	\end{equation}
	and by applying the same process
	\begin{equation}
		\begin{split}
			\vt{C}&=(\vt{B}^{-1}+\vt{A}^{-1})^{-1}
			=(\vt{Z}+\vt{U}\vt{W}\vt{V}^\trans)^{-1}
			=(\vt{B}^{-1}+\vt{I}\vt{A}^{-1}\vt{I})^{-1} \\
			&=\vt{B}-\vt{B}(\vt{A}+\vt{B})^{-1}\vt{B} \\
			&=\vt{A}-\vt{A}(\vt{A}+\vt{B})^{-1}\vt{A} \\
			\vt{C}&=(\vt{A}^{-1}+\vt{B}^{-1})^{-1}
		\end{split}
	\end{equation}

\item Show that
				\begin{equation}
								K^{-1} = (2\pi)^{-D/2}|\vt{A}+\vt{B}|^{-1/2}
								\exp\big( -\frac{1}{2}(\vt{a}-\vt{b})^\trans
								(\vt{A}+\vt{B})^{-1} (\vt{a}-\vt{b})\big)
				\end{equation}

	\begin{equation}
		\begin{split}
						K^{-1}&=(2\pi)^{-\frac{D}{2}}\frac{|\vt{C}|^{-\frac{1}{2}}}{|\vt{AB}|^{-\frac{1}{2}}}
						\exp\left\{
										\frac{1}{2}\vt{c}^\trans\vt{C}^{-1}\vt{c}
										-\frac{1}{2}\vt{a}^\trans\vt{A}^{-1}\vt{a}
										-\frac{1}{2}\vt{b}^\trans\vt{B}^{-1}\vt{b}
						\right\} \\
						&=(2\pi)^{-\frac{D}{2}}\frac{|\vt{C}|^{-\frac{1}{2}}}{|\vt{AB}|^{-\frac{1}{2}}}
						\exp\left\{
										\frac{1}{2}\left[
														(\vt{A}^{-1}+\vt{B}^{-1})^{-1}(\vt{A}^{-1}\vt{a}-\vt{B}^{-1}\vt{b})
										\right]^\trans(\vt{A}^{-1}\vt{a}+\vt{B}^{-1}\vt{b})
										-\frac{1}{2}\vt{a}^\trans\vt{A}^{-1}\vt{a}
										-\frac{1}{2}\vt{b}^\trans\vt{B}^{-1}\vt{b}
						\right\} \\
						&=(2\pi)^{-\frac{D}{2}}\frac{|\vt{C}|^{-\frac{1}{2}}}{|\vt{AB}|^{-\frac{1}{2}}}
						\exp\left\{
										\frac{1}{2}
										(\vt{A}^{-1}\vt{a}-\vt{B}^{-1}\vt{b})^\trans
										(\vt{A}^{-1}+\vt{B}^{-1})^{-1}
										(\vt{A}^{-1}\vt{a}+\vt{B}^{-1}\vt{b})
										-\frac{1}{2}\vt{a}^\trans\vt{A}^{-1}\vt{a}
										-\frac{1}{2}\vt{b}^\trans\vt{B}^{-1}\vt{b}
						\right\} \\
						&=(2\pi)^{-\frac{D}{2}}\frac{|\vt{C}|^{-\frac{1}{2}}}{|\vt{AB}|^{-\frac{1}{2}}}
						\exp\left\{
										\frac{1}{2}
										(\vt{a}^\trans\vt{A}^{-1}-\vt{b}^\trans\vt{B}^{-1})
										(\vt{A}^{-1}+\vt{B}^{-1})^{-1}
										(\vt{A}^{-1}\vt{a}+\vt{B}^{-1}\vt{b})
										-\frac{1}{2}\vt{a}^\trans\vt{A}^{-1}\vt{a}
										-\frac{1}{2}\vt{b}^\trans\vt{B}^{-1}\vt{b}
						\right\} \\
						&=(2\pi)^{-\frac{D}{2}}\frac{|\vt{C}|^{-\frac{1}{2}}}{|\vt{AB}|^{-\frac{1}{2}}}
						\exp\left\{
										\frac{1}{2}
										(\vt{a}^\trans\vt{A}^{-1}-\vt{b}^\trans\vt{B}^{-1})
										[\vt{A}-\vt{A}(\vt{A}+\vt{B})^{-1}\vt{A}]
										(\vt{A}^{-1}\vt{a}+\vt{B}^{-1}\vt{b})
										\right.\nonumber \\
										&\qquad\left.-\frac{1}{2}\vt{a}^\trans\vt{A}^{-1}\vt{a}
										-\frac{1}{2}\vt{b}^\trans\vt{B}^{-1}\vt{b}
						\right\}
		\end{split}
	\end{equation}
	then developing the exponential term expanding the product
	\begin{equation}
		\begin{split}
						&=\frac{1}{2}\vt{a}^\trans\vt{A}^{-1}\vt{a}
						+\frac{1}{2}\vt{a}^\trans\vt{B}^{-1}\vt{b}
						-\frac{1}{2}\vt{a}^\trans(\vt{A}+\vt{B})^{-1}\vt{a}
						-\frac{1}{2}\vt{a}^\trans(\vt{A}+\vt{B})^{-1}\vt{A}\vt{B}^{-1}\vt{b}
						+\frac{1}{2}\vt{b}^\trans\vt{B}^{-1}\vt{a}
						+\frac{1}{2}\vt{B}^{-1}\vt{A}\vt{B}^{-1}\vt{b} \\
						&\qquad-\frac{1}{2}\vt{b}^\trans\vt{B}^{-1}\vt{A}(\vt{A}+\vt{B})^{-1}\vt{a}
						-\frac{1}{2}\vt{b}^\trans\vt{B}^{-1}\vt{A}(\vt{A}+\vt{B})^{-1}\vt{A}\vt{B}^{-1}\vt{b}
						-\frac{1}{2}\vt{a}^\trans\vt{A}^{-1}\vt{a}
						-\frac{1}{2}\vt{b}^\trans\vt{B}^{-1}\vt{b} \\
						&=
		\end{split}
	\end{equation}

\end{enumerate}
\end{problem}


\begin{problem}
Tossing a biased coin with probability that it comes up heads is $\mu$. 
\begin{enumerate}
\item We toss the coin 3 times and it all comes up with heads.
				How likely is that in the next toss, the coin comes up
				with head according to MLE?

	We can model a single coin flip with a Bernoulli distribution where 1 means
	heads and 0 means tails
	$$X_i \sim \text{Ber}(x|\mu)\begin{cases}\mu & x=1 \\ 1-\mu & x=0\end{cases}$$
	so that the mean according to the MLE for $\mathcal{X}=\{X_1=1,X_2=1,X_3=1\}$ is given by
	$$\mu_\text{ML}=\frac{m}{N}=\frac{1}{N}\sum_{i=1}^N X_i=1$$

	We can then use $\mu_\text{ML}$ to predict the probability that the
	4th coin toss will be head as follows
	$$p(X_4=1|\mu_\text{ML})=\mu_\text{ML}=1$$

\item Suppose that the prior $\mu \sim \text{Beta}(\mu|a,b)$.
				What is the probability  that the coin comes up with
				head in the 4th toss?

	First we need to compute the posterior mean
	\begin{equation}
		\begin{split}
			p(\mu|\mathcal{X})&=p(\mathcal{X}|\mu)p(\mu) \\
			&=p(X_1=1,X_2=1,X_3=1|\mu)p(\mu|a,b) \\
			&=\prod_{i=1}^3p(X_i=1|\mu)p(\mu|a,b) \\
			&=\mu^3\frac{\Gamma(a+b)}{\Gamma(a)+\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1} \\
			&=\frac{\Gamma(a+b)}{\Gamma(a)+\Gamma(b)}\mu^{a+2}(1-\mu)^{b-1}
		\end{split}
	\end{equation}
	concluding that it is also the probability of the 4th coin flip coming up
	heads.

\item Suppose that we observe $m$ times that the coin lands
				heads and $l$ times that it lands tails. Show that the
				posterior mean lies between the prior mean and
				$\mu_{\text{MLE}}$.

	We can model the entire experiment as a Binomial distribution
	$X \sim \text{Bin}(x|m+l,\mu)$ so we have that the posterior mean is
	\begin{equation}
		\begin{split}
			p(\mu|X)&=p(x|\mu)p(\mu) \\
			&=\frac{(l+m)!}{l!m!}\mu^m(1-\mu)^l\frac{\Gamma(a+b)}{\Gamma(a)+\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1} \\
			&=\frac{(l+m)!}{l!m!}\frac{\Gamma(a+b)}{\Gamma(a)+\Gamma(b)}\mu^{m+a-1}(1-\mu)^{l+b-1} \\
			&\propto
			\frac{\Gamma(a+b+m+l)}{\Gamma(a+m)+\Gamma(b+l)}\mu^{m+a-1}(1-\mu)^{l+b-1} \\
			&=\text{Beta}(\mu|a+m,b+l)
		\end{split}
	\end{equation}

	Because the posterior mean is a probability distribution, its value lies
	between 0 and 1, so it's less or equal than $\mu_\text{MLE}$. We can then also
	note that the terms with $\mu$ of the prior are proportional to $a-1$ and
	$b-1$, while for the posterior they are $m+a-1$ and $l+b-1$, so the $\mu$
	terms of the posterior grow faster than the prior. Moreover we know that the
	Gamma function is monotonically increasing for $a,b>0$, so we can conclude
	that the posterior over the mean is greater or equal to the prior.

\end{enumerate}
\end{problem}

\begin{extraproblem}
Derive mean, covariance, and mode of multivariate Student's
t-distribution.
\end{extraproblem}



\end{document}

