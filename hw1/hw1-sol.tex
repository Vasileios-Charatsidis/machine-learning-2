%        File: hw1-sol.tex
%     Created: gio mar 31 12:00  2016 C
% Last Change: gio mar 31 12:00  2016 C
%
\documentclass{amsmlaj}

\begin{document}

\lecturesol{Homework 1}{Ke Tran}{m.k.tran@uva.nl}
{April 05, 2016}{Andrea Jemmett}{11162929}{andreajemmett@gmail.com}{N/A}


\begin{problem}
Consider two random vectors $\vt{x} \in \R^n$ and $\vt{z} \in \R^n$ having
Gaussian distribution  $p(\vt{x}) = \distNorm(\vt{x} | \vt{\mu}_{\vt{x}},
\vt{\Sigma}_{\vt{x}})$ and  $p(\vt{z}) = \distNorm(\vt{z} | \vt{\mu}_{\vt{z}},
\vt{\Sigma}_{\vt{z}})$.  Consider random vector $\vt{y} = \vt{x} + \vt{z}$.
Derive mean and covariance of $p(\vt{y})$.
\end{problem}

\begin{problem}
Given a set of $N$ observations $\mathcal{X} = \{x_1,\dots,x_N\}$.
Assume that $x_i \sim \distNorm(\mu,\sigma^2)$ where $\sigma^2$ is known
and $\mu \sim \distNorm(\mu_0,\sigma_0^2)$. 
\begin{enumerate}
\item Write down the likelihood of the data
				$p(\mathcal{X}|\mu,\sigma^2)$;
		
	\begin{equation}
		p(\mathcal{X}|\mu,\sigma^2)=\prod_{i=1}^{N}p(x_i|\mu,\sigma^2)
	\end{equation}

\item Write down the posterior $p(\mu|\mathcal{X},\sigma^2,
				\mu_0,\sigma_0^2)$;

	\begin{equation}
		\begin{split}
		p(\mu|\mathcal{X},\sigma^2,\mu_0,\sigma_0^2)
			&=p(\mathcal{X}|\mu,\sigma^2)p(\mu|\mu_0,\sigma_0^2) \\
			&=\prod_{i=1}^{N}p(x_i|\mu,\sigma^2)p(\mu|\mu_0,\sigma_0^2)
		\end{split}
	\end{equation}

\item Show that $p(\mu|\mathcal{X},\sigma^2, \mu_0,\sigma_0^2)$
				is a Gaussian distribution
				$\distNorm(\mu|\mu_N,\sigma_N^2)$ and find the values of
				$\mu_N$ and $\sigma_N^2$;

	\begin{equation}
		\begin{split}
		p(\mu|\mathcal{X},\sigma^2, \mu_0,\sigma_0^2)
		&=\prod_{i=1}^{N}\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left\{ -\frac{1}{2\sigma^2}(x_i-\mu)^2
		\right\} \frac{1}{\sqrt{2\pi\sigma_0^2}}\exp\left\{
		-\frac{1}{2\sigma_0^2}(\mu-\mu_0)^2 \right\} \\
		&=\frac{1}{2\pi\sqrt{\sigma^2\sigma_0^2}}\exp\left\{
			-\frac{1}{2\sigma^2}\sum_{i=1}^N (x_i-\mu)^2
			-\frac{1}{2\sigma_0^2}(\mu-\mu_0)^2
		\right\} \\
		&=\frac{1}{2\pi\sqrt{\sigma^2\sigma_0^2}}\exp\left\{
			-\frac{1}{2\sigma^2}\left(
			\sum_{i=1}^N x_i^2 - \sum_{i=1}^N 2x_i\mu + N\mu^2
			\right) -
			\frac{1}{2\sigma_0^2} (\mu^2 - 2\mu\mu_0 + \mu_0^2)
		\right\} \\
		&=\frac{1}{2\pi\sqrt{\sigma^2\sigma_0^2}}\exp\left\{ 
			-\frac{\mu^2}{2}\left( \frac{1}{\sigma_0^2}+\frac{N}{\sigma^2} \right)
			+
			\mu\left( 
			\frac{1}{\sigma^2}\sum_{i=1}^N x_i +
			\frac{\mu_0}{\sigma_0^2}
			\right) + \text{const}
		\right\} \\
		&=\frac{1}{2\pi\sqrt{\sigma^2\sigma_0^2}}\exp\left\{ 
			-\underbrace{\frac{\mu^2}{2}\left(
				\frac{1}{\sigma_0^2}+\frac{N}{\sigma^2}
			\right)}_{-\frac{1}{2\sigma_N^2}}
			+
			\mu\underbrace{\left( 
			\frac{N}{\sigma^2} \mu_\text{ML} +
			\frac{\mu_0}{\sigma_0^2}
			\right)}_{\frac{1}{\sigma_N^2}\mu_N} + \text{const}
		\right\}
		\end{split}
	\end{equation}
	where $\mu_\text{ML}=\frac{1}{N}\sum_{i=1}^N x_i$ is the sample mean, \textit{const} are terms
	not dependent on $\mu$ and it is a Gaussian
	distribution (because of the form of coefficients entering quadratic $\mu^2$
	and linear $\mu$ terms) with mean and variance given by
	\begin{equation}
		\begin{split}
			\frac{1}{\sigma_N^2}&=\frac{1}{\sigma_0^2}+\frac{N}{\sigma^2} \\
			\mu_N&=\left(
					\frac{N\mu_\text{ML}}{\sigma^2}+\frac{\mu_0}{\sigma_0^2}
				\right)\sigma_N^2 \\
				&=\left(
				\frac{N\mu_\text{ML}}{\sigma^2}+\frac{\mu_0}{\sigma_0^2}
				\right)\frac{\sigma_0^2\sigma^2}{\sigma^2+N\sigma_0^2} \\
				&=\frac{\sigma^2}{N\sigma_0^2+\sigma^2}\mu_0  +
					\frac{N\sigma_0^2}{N\sigma_0^2+\sigma^2}\mu_\text{ML}
		\end{split}
	\end{equation}

\item Derive the maximum a posterior solution for $\mu$;

	\begin{equation}
		\begin{split}
						\frac{\partial}{\partial\mu}\log p(\mu|\mathcal{X},\sigma^2,\mu_0,\sigma_0^2)
						&=\frac{\partial}{\partial\mu}\log\left(\frac{1}{2\pi\sqrt{\sigma^2\sigma_0^2}}\right)
						-\frac{\partial}{\partial\mu}\left[\frac{1}{2\sigma^2}\sum_{i=1}^N(x_i-\mu)^2
						\right]-\frac{\partial}{\partial\mu}\left[
										\frac{1}{2\sigma_0^2}(\mu-\mu_0)^2
						\right] \\
						&=\frac{N}{2\sigma^2}\frac{\partial}{\partial\mu}\mu^2+
						\frac{1}{2\sigma_0^2}\frac{\partial}{\partial\mu}\mu^2-
						\frac{1}{\sigma^2}\sum_{i=1}^N x_i\frac{\partial}{\partial\mu}\mu-
						\frac{\mu_0}{\sigma_0^2}\frac{\partial}{\partial\mu}\mu+
						\frac{\partial}{\partial\mu}\left[ 
										\frac{1}{2\sigma^2}\sum_{i=1}^N x_i^2 +
										\frac{1}{2\sigma_0^2}\mu_0^2
						\right] \\
						&=\frac{N}{\sigma^2}\mu +
						\frac{1}{\sigma_0^2}\mu -
						\frac{N\mu_\text{ML}}{\sigma^2} -
						\frac{\mu_0}{\sigma_0^2} \\
						&=
						\frac{N\sigma_0^2+\sigma^2}{\sigma^2\sigma_0^2}\mu -
						\frac{N\sigma_0^2\mu_\text{ML}+\mu_0\sigma^2}{\sigma^2\sigma_0^2}=0
		\end{split}
	\end{equation}
	we can then solve for $\mu$
	\begin{equation}
		\begin{split}
					\hat{\mu}_\text{MAP}&=
						\frac{N\sigma_0^2\mu_\text{ML}+\mu_0\sigma^2}{\sigma^2\sigma_0^2}
						\frac{\sigma^2\sigma_0^2}{N\sigma_0^2+\sigma^2} \\
						&=\frac{N\sigma_0^2\mu_\text{ML}+\sigma^2\mu_0}{N\sigma_0^2+\sigma^2} \\
						&=\frac{N\sigma_0^2}{N\sigma_0^2+\sigma^2}\mu_\text{ML}
						+\frac{\sigma^2}{N\sigma_0^2+\sigma^2}\mu_0 \\
						&=\mu_N
		\end{split}
	\end{equation}

\item Derive expressions for sequential update of $\mu_N$ and
				$\sigma_N^2$;

	First define $\mu_N^{(N)}$ as the estimated $\mu_N$ using $N$ samples.
	Then we can write:
	\begin{equation}
		\begin{split}
			\mu_N^{(N)}&=
			\frac{\sigma^2}{N\sigma_0^2+\sigma^2}\mu_0+
			\frac{N\sigma_0^2}{N\sigma_0^2+\sigma^2}\mu_\text{ML} \\
			&=\frac{\sigma^2}{N\sigma_0^2+\sigma^2}\mu_0+
			\frac{N\sigma_0^2}{N\sigma_0^2+\sigma^2}\frac{1}{N}\sum_{i=1}^N x_i \\
			&=\frac{\sigma^2}{N\sigma_0^2+\sigma^2}\mu_0+
			\frac{\sigma_0^2}{N\sigma_0^2+\sigma^2}\sum_{i=1}^N x_i \\
			&=\frac{\sigma^2}{N\sigma_0^2+\sigma^2}\mu_0+
			\frac{\sigma_0^2}{N\sigma_0^2+\sigma^2}\left(
				x_N + \sum_{i=1}^{N-1}x_i
			\right) \\
			&=\frac{\sigma^2}{N\sigma_0^2+\sigma^2}\mu_0+
			\frac{\sigma_0^2}{N\sigma_0^2+\sigma^2}\left[
				x_N + (N-1)\mu_\text{ML}^{(N-1)}
			\right] \\
			&=\frac{\sigma^2}{N\sigma_0^2+\sigma^2}\mu_0+
			\frac{(N+1)\sigma_0^2}{N\sigma_0^2+\sigma^2}\mu_\text{ML}^{(N-1)}+
			\frac{\sigma_0^2}{N\sigma_0^2+\sigma^2}x_N \\
			&=\frac{1}{N\sigma_0^2+\sigma^2}(\sigma^2\mu_0+(N-1)\sigma_0^2\mu_\text{ML}^{(N-1)})
			+\frac{\sigma_0^2}{N\sigma_0^2+\sigma^2}x_N \\
			&=\frac{(N-1)\sigma_0^2+\sigma^2}{N\sigma_0^2+\sigma^2}\left(
			\frac{\sigma^2}{(N-1)\sigma_0^2+\sigma^2}\mu_0+
			\frac{(N-1)\sigma_0^2}{(N-1)\sigma_0^2+\sigma^2}\mu_\text{ML}^{(N-1)}
			\right)+\frac{\sigma_0^2}{N\sigma_0^2+\sigma^2}x_N \\
			&=\frac{(N-1)\sigma_0^2+\sigma^2}{N\sigma_0^2+\sigma^2}\mu_N^{(N-1)}+
			\frac{\sigma_0^2}{N\sigma_0^2+\sigma^2}x_N
		\end{split}
	\end{equation}

\item Derive the same results (as in 5) starting from the
				posterior distribution $p(\mu|x_1,\dots,x_{N-1})$, and
				multiplying by the likelihood function $p(x_N |\mu) =
				\distNorm(x_N |\mu,\sigma^2)$.
\end{enumerate}
\end{problem}

\begin{problem}
Consider a $D$-dimensional Gaussian random variable $\vt{x}$ with
distribution $\distNorm(\vt{x}|\vt{\mu},\vt{\Sigma})$ in which the
covariance $\vt{\Sigma}$ is known and for which we wish to infer the
mean $\vt{\mu}$ from a set of observations
$\mathcal{X}=\{\vt{x}_1,\dotsc,\vt{x}_N\}$. 
\begin{enumerate}
\item Write down the likelihood of the data
				$p(\mathcal{X}|\vt{\mu},\vt{\Sigma})$;

	\begin{equation}
		\begin{split}
			p(\mathcal{X}|\vt{\mu},\vt{\Sigma})&=
			\prod_{i=1}^D p(\vt{x}_i|\vt{\mu},\vt{\Sigma}) \\
			&=\frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{|\Sigma|^\frac{1}{2}}\prod_{i=1}^D
			\exp\left\{ \frac{1}{2}(\vt{x}_i-\vt{\mu})^T\Sigma^{-1}(\vt{x}_i-\vt{\mu})
			\right\} \\
			&=\frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{|\Sigma|^\frac{1}{2}}\exp\left\{
				-\sum_{i=1}^D\frac{1}{2}(\vt{x}_i-\vt{\mu})^T\Sigma^{-1}(\vt{x}_i-\vt{\mu})
			\right\}
		\end{split}
	\end{equation}

\item Given a prior distribution
				$p(\vt{\mu})=\distNorm(\vt{\mu}|\vt{\mu}_0,\vt{\Sigma}_0)$,
				find the corresponding posterior distribution
				$p(\vt{\mu}|\mathcal{X},\vt{\Sigma},\vt{\mu}_0,\vt{\Sigma}_0)$.
\item Show that the posterior
				$p(\vt{\mu}|\mathcal{X},\vt{\Sigma},\vt{\mu}_0,\vt{\Sigma}_0)$
				is a Gaussian distribution with mean $\vt{\mu}_N$ and
				covariance $\vt{\Sigma}_N$
\item Find $\vt{\mu}_N$ and $\vt{\Sigma}_N$
\end{enumerate}
\end{problem}
%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
\begin{problem}

\begin{enumerate}
\item Show that the product of two Gaussians gives another
				(un-normalized) Gaussian
				$
				\distNorm(\vt{x}|\vt{a},\vt{A})\distNorm(\vt{x}|\vt{b},\vt{B})
				= K^{-1}\distNorm(\vt{x}|\vt{c},\vt{C})
				$
				where $\vt{c}=\vt{C}(\vt{A}^{-1}\vt{a} +
				\vt{B}^{-1}\vt{b})$ and
				$\vt{C}=(\vt{A}^{-1}+\vt{B}^{-1})^{-1}$.
\item Using the \emph{matrix inversion lemma}, also known as the
				the Woodbury, Sherman \& Morrison formula: 
				\begin{equation}
								(\vt{Z}+\vt{U}\vt{W}\vt{V}^\trans)^{-1} =
								\vt{Z}^{-1} -
								\vt{Z}^{-1}\vt{U}(\vt{W}^{-1}+\vt{V}^\trans
								\vt{Z}^{-1}\vt{U})^{-1}\vt{V}^\trans \vt{Z}^{-1}
				\end{equation}
				Proof that $\vt{C} =
				(\vt{A}^{-1}+\vt{B}^{-1})^{-1}=\vt{A} -
				\vt{A}(\vt{A}+\vt{B})^{-1}\vt{A} = \vt{B} -
				\vt{B}(\vt{A}+\vt{B})^{-1}\vt{B}$
				
\item Show that
				\begin{equation}
								K^{-1} = (2\pi)^{-D/2}|\vt{A}+\vt{B}|^{-1/2}
								\exp\big( -\frac{1}{2}(\vt{a}-\vt{b})^\trans
								(\vt{A}+\vt{B})^{-1} (\vt{a}-\vt{b})\big)
				\end{equation}
\end{enumerate}
\end{problem}


\begin{problem}
Tossing a biased coin with probability that it comes up heads is $\mu$. 
\begin{enumerate}
\item We toss the coin 3 times and it all comes up with heads.
				How likely is that in the next toss, the coin comes up
				with head according to MLE?
\item Suppose that the prior $\mu \sim \text{Beta}(\mu|a,b)$.
				What is the probability  that the coin comes up with
				head in the 4th toss?
\item Suppose that we observe $m$ times that the coin lands
				heads and $l$ times that it lands tails. Show that the
				posterior mean lies between the prior mean and
				$\mu_{\text{MLE}}$.
\end{enumerate}
\end{problem}

\begin{extraproblem}
Derive mean, covariance, and mode of multivariate Student's
t-distribution.
\end{extraproblem}



\end{document}

