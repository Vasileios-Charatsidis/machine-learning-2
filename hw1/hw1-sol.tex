%        File: hw1-sol.tex
%     Created: gio mar 31 12:00  2016 C
% Last Change: gio mar 31 12:00  2016 C
%
\documentclass{amsmlaj}

\begin{document}

\lecturesol{Homework 1}{Ke Tran}{m.k.tran@uva.nl}
{April 05, 2016}{Andrea Jemmett}{11162929}{andreajemmett@gmail.com}{N/A}


\begin{problem}
Consider two random vectors $\vt{x} \in \R^n$ and $\vt{z} \in \R^n$ having
Gaussian distribution  $p(\vt{x}) = \distNorm(\vt{x} | \vt{\mu}_{\vt{x}},
\vt{\Sigma}_{\vt{x}})$ and  $p(\vt{z}) = \distNorm(\vt{z} | \vt{\mu}_{\vt{z}},
\vt{\Sigma}_{\vt{z}})$.  Consider random vector $\vt{y} = \vt{x} + \vt{z}$.
Derive mean and covariance of $p(\vt{y})$.
\end{problem}

\begin{problem}
Given a set of $N$ observations $\mathcal{X} = \{x_1,\dots,x_N\}$.
Assume that $x_i \sim \distNorm(\mu,\sigma^2)$ where $\sigma^2$ is known
and $\mu \sim \distNorm(\mu_0,\sigma_0^2)$. 
\begin{enumerate}
\item Write down the likelihood of the data
				$p(\mathcal{X}|\mu,\sigma^2)$;
		
	\begin{equation}
		p(\mathcal{X}|\mu,\sigma^2)=\prod_{i=1}^{N}p(x_i|\mu,\sigma^2)
	\end{equation}

\item Write down the posterior $p(\mu|\mathcal{X},\sigma^2,
				\mu_0,\sigma_0^2)$;

	\begin{equation}
		\begin{split}
		p(\mu|\mathcal{X},\sigma^2,\mu_0,\sigma_0^2)
			&=p(\mathcal{X}|\mu,\sigma^2)p(\mu|\mu_0,\sigma_0^2) \\
			&=\prod_{i=1}^{N}p(x_i|\mu,\sigma^2)p(\mu|\mu_0,\sigma_0^2)
		\end{split}
	\end{equation}

\item Show that $p(\mu|\mathcal{X},\sigma^2, \mu_0,\sigma_0^2)$
				is a Gaussian distribution
				$\distNorm(\mu|\mu_N,\sigma_N^2)$ and find the values of
				$\mu_N$ and $\sigma_N^2$;

	\begin{equation}
		\begin{split}
		p(\mu|\mathcal{X},\sigma^2, \mu_0,\sigma_0^2)
		&=\prod_{i=1}^{N}\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left\{ \frac{1}{2\sigma^2}(x_i-\mu)^2
		\right\} \frac{1}{\sqrt{2\pi\sigma_0^2}}\exp\left\{
		\frac{1}{2\sigma_0^2}(\mu-\mu_0)^2 \right\} \\
		&=\frac{1}{2\pi\sqrt{\sigma^2\sigma_0^2}}\exp\left\{
			\frac{1}{2\sigma^2}\sum_{i=1}^N (x_i-\mu)^2
			+\frac{1}{2\sigma_0^2}(\mu-\mu_0)^2
		\right\} \\
		&=\frac{1}{2\pi\sqrt{\sigma^2\sigma_0^2}}\exp\left\{
			\frac{1}{2\sigma^2}\left(
			\sum_{i=1}^N x_i^2 + \sum_{i=1}^N 2x_i\mu + N\mu^2
			\right) +
			\frac{1}{2\sigma_0^2} (\mu^2 + \mu_0^2 + 2\mu\mu_0)
		\right\} \\
		&=\frac{1}{2\pi\sqrt{\sigma^2\sigma_0^2}}\exp\left\{ 
			\frac{\mu^2}{2}\left( \frac{1}{\sigma_0^2}+\frac{N}{\sigma^2} \right) +
			\mu\left( 
			\frac{1}{\sigma^2}\sum_{i=1}^N x_i +
			\frac{\mu_0}{\sigma_0^2}
			\right) + \text{const}
		\right\} \\
		&=\frac{1}{2\pi\sqrt{\sigma^2\sigma_0^2}}\exp\left\{ 
			\frac{\mu^2}{2}\underbrace{\left(
				\frac{1}{\sigma_0^2}+\frac{N}{\sigma^2}
			\right)}_{\frac{1}{\sigma_N^2}} +
			\mu\underbrace{\left( 
			\frac{N}{\sigma^2} \mu_\text{ML} +
			\frac{\mu_0}{\sigma_0^2}
			\right)}_{\frac{1}{\sigma_N^2}\mu_N} + \text{const}
		\right\}
		\end{split}
	\end{equation}
	where $\mu_\text{ML}=\frac{1}{N}\sum_{i=1}^N x_i$, \textit{const} are terms
	not dependent of $\mu$ and it is a Gaussian
	distribution with mean and variance given by
	\begin{equation}
		\begin{split}
			\mu_N&=\frac{\sigma^2}{N\sigma_0^2+\sigma^2}\mu_0  +
				\frac{N\sigma_0^2}{N\sigma_0^2+\sigma^2}\mu_\text{ML} \\
			\frac{1}{\sigma_N^2}&=\frac{1}{\sigma_0^2}+\frac{N}{\sigma^2}
		\end{split}
	\end{equation}

\item Derive the maximum a posterior solution for $\mu$;
\item Derive expressions for sequential update of $\mu_N$ and
				$\sigma_N^2$;
\item Derive the same results (as in 5) starting from the
				posterior distribution $p(\mu|x_1,\dots,x_{N-1})$, and
				multiplying by the likelihood function $p(x_N |\mu) =
				\distNorm(x_N |\mu,\sigma^2)$.
\end{enumerate}
\end{problem}

\begin{problem}
Consider a $D$-dimensional Gaussian random variable $\vt{x}$ with
distribution $\distNorm(\vt{x}|\vt{\mu},\vt{\Sigma})$ in which the
covariance $\vt{\Sigma}$ is known and for which we wish to infer the
mean $\vt{\mu}$ from a set of observations
$\mathcal{X}=\{\vt{x}_1,\dotsc,\vt{x}_N\}$. 
\begin{enumerate}
\item Write down the likelihood of the data
				$p(\mathcal{X}|\vt{\mu},\vt{\Sigma})$;
\item Given a prior distribution
				$p(\vt{\mu})=\distNorm(\vt{\mu}|\vt{\mu}_0,\vt{\Sigma}_0)$,
				find the corresponding posterior distribution
				$p(\vt{\mu}|\mathcal{X},\vt{\Sigma},\vt{\mu}_0,\vt{\Sigma}_0)$.
\item Show that the posterior
				$p(\vt{\mu}|\mathcal{X},\vt{\Sigma},\vt{\mu}_0,\vt{\Sigma}_0)$
				is a Gaussian distribution with mean $\vt{\mu}_N$ and
				covariance $\vt{\Sigma}_N$
\item Find $\vt{\mu}_N$ and $\vt{\Sigma}_N$
\end{enumerate}
\end{problem}
%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
\begin{problem}

\begin{enumerate}
\item Show that the product of two Gaussians gives another
				(un-normalized) Gaussian
				$
				\distNorm(\vt{x}|\vt{a},\vt{A})\distNorm(\vt{x}|\vt{b},\vt{B})
				= K^{-1}\distNorm(\vt{x}|\vt{c},\vt{C})
				$
				where $\vt{c}=\vt{C}(\vt{A}^{-1}\vt{a} +
				\vt{B}^{-1}\vt{b})$ and
				$\vt{C}=(\vt{A}^{-1}+\vt{B}^{-1})^{-1}$.
\item Using the \emph{matrix inversion lemma}, also known as the
				the Woodbury, Sherman \& Morrison formula: 
				\begin{equation}
								(\vt{Z}+\vt{U}\vt{W}\vt{V}^\trans)^{-1} =
								\vt{Z}^{-1} -
								\vt{Z}^{-1}\vt{U}(\vt{W}^{-1}+\vt{V}^\trans
								\vt{Z}^{-1}\vt{U})^{-1}\vt{V}^\trans \vt{Z}^{-1}
				\end{equation}
				Proof that $\vt{C} =
				(\vt{A}^{-1}+\vt{B}^{-1})^{-1}=\vt{A} -
				\vt{A}(\vt{A}+\vt{B})^{-1}\vt{A} = \vt{B} -
				\vt{B}(\vt{A}+\vt{B})^{-1}\vt{B}$
				
\item Show that
				\begin{equation}
								K^{-1} = (2\pi)^{-D/2}|\vt{A}+\vt{B}|^{-1/2}
								\exp\big( -\frac{1}{2}(\vt{a}-\vt{b})^\trans
								(\vt{A}+\vt{B})^{-1} (\vt{a}-\vt{b})\big)
				\end{equation}
\end{enumerate}
\end{problem}


\begin{problem}
Tossing a biased coin with probability that it comes up heads is $\mu$. 
\begin{enumerate}
\item We toss the coin 3 times and it all comes up with heads.
				How likely is that in the next toss, the coin comes up
				with head according to MLE?
\item Suppose that the prior $\mu \sim \text{Beta}(\mu|a,b)$.
				What is the probability  that the coin comes up with
				head in the 4th toss?
\item Suppose that we observe $m$ times that the coin lands
				heads and $l$ times that it lands tails. Show that the
				posterior mean lies between the prior mean and
				$\mu_{\text{MLE}}$.
\end{enumerate}
\end{problem}

\begin{extraproblem}
Derive mean, covariance, and mode of multivariate Student's
t-distribution.
\end{extraproblem}



\end{document}

