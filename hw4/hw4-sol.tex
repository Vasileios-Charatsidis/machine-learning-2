\documentclass{amsmlaj}
\begin{document}
\lecturesol{Homework 4}{Ke Tran}{m.k.tran@uva.nl}{May 03, 2016}
{Andrea Jemmett}{11162929}{andrea.jemmett@student.uva.nl}{N/A}
\noindent {\footnotesize You are allowed to  discuss with your colleagues but you should write the answers in \emph{your own words}. If you discuss with others, write down the name of your collaborators on top of the first page. No points will be deducted for collaborations. If we find similarities in solutions beyond the listed collaborations we will consider it as cheating.}

\noindent {\footnotesize We will not accept any late submissions under any circumstances. The solutions to the previous homework will be handed out in the class at the beginning of the next homework session. After this point, late submissions will be automatically graded zero.}
%\make title

\begin{problem}
Consider a Gaussian mixture model
$$
p(\vt{x}) = \sum_{k=1}^K  \pi_k \distNorm(\vt{x}|\vt{\mu}_k,\vt{\Sigma}_k)
$$
\begin{enumerate}
\item Given the expected value of the complete-data log-likelihood (9.40 in Bishop's book)
$$
\Ex_{\text{posterior}}[\ln p(\vt{X},\vt{Z}|\vt{\mu}, \vt{\Sigma},\vt{\pi})] = \sum_{n=1}^N\sum_{k=1}^K \gamma(z_{nk}) \left\{ \ln\pi_k + \ln\distNorm(\vt{x}_n|\vt{\mu}_k,\vt{\Sigma}_k)\right\} \label{eq:ex_post}
$$
Derive update rules for $\vt{\pi}$, $\vt{\mu}$ and $\vt{\Sigma}$. 

\item Consider a special case of the model above, in which the covariance matrices $\vt{\Sigma}_k$ of the components are all constrained to have a common value $\vt{\Sigma}$. Derive EM equations for maximizing the likelihood function under such a model.
\end{enumerate}

\end{problem}

\begin{problem}
\begin{figure}[htbp]
\centering
\begin{tikzpicture}
  % Define nodes
  \node[obs]                               (x) {$\vt{x}$};
  \node[latent, above=1cm of x] (z) {$\vt{z}$};
 
  \node[const, right=1cm of z]            (t) {$\vt{\theta}$};

  % Connect the nodes
  \edge {z,t} {x} ; %
  \edge {t}{z} ;

  % Plates
  \plate[inner sep=0.5cm] {xz} {(x)(z)} {$N$} ;
\end{tikzpicture}
\caption{A simple generative model.}
\label{fig:gm}
\end{figure}
Suppose we wish to use the EM algorithm to maximize the posterior distribution $p(\vt{\theta}|\vt{X})$ for a model (Figure \ref{fig:gm}) containing latent variables $\vt{z}$ and observed variables $\vt{x}$. Show that the E step remains the same as in the maximum likelihood case, where as in the M step, the quantity to be maximized is
$$
\sum_{\vt{z}} p(\vt{Z}|\vt{X},\vt{\theta}^{\text{old}})\ln p(\vt{X},\vt{Z}|\vt{\theta}) + \ln p(\vt{\theta})
$$
\end{problem}

\begin{problem}
\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
\node[obs]                   (x)      {$\vt{x}_n$} ;
\node[latent, above=of x] (z) {$\vt{z_n}$};
\node[latent, above=of z] (pi) {$\vt{\pi}$};
\node[const, right=of pi] (alpha) {$\vt{\alpha}$};
\node[latent, left=of z] (mu) {$\vt{\mu}_k$};
\node[const, left=0.5cm of mu] (ab) {$a,b$};
\node[const, right=0.5cm of x] (N) {$N$};
\plate {plate_mu} { (mu) } {$K$};
\plate {plate_z} { (z)(x)(N) } {};
\edge[->]{ab}{mu};
\edge[->]{mu}{x};
\edge[->]{z}{x};
\edge[->]{pi}{z};
\edge[->]{alpha}{pi};
\end{tikzpicture}
\caption{Mixtures of Bernoulli distribution}
\label{fig:mixbern}
\end{center}
\end{figure}
\begin{eqnarray}
\vt{\pi}|\vt{\alpha} & \sim & \text{Dir}(\vt{\pi}|\vt{\alpha}) \nonumber\\
\vt{z}_n | \vt{\pi} & \sim & \text{Mult}(\vt{z}_n | \vt{\pi}) \nonumber\\
\vt{\mu}_k|a_k, b_k & \sim & \text{Beta}(\vt{\mu}_k|a_k, b_k) \nonumber\\
\vt{x}_n|\vt{z}_n, \vt{\mu}=\{\vt{\mu}_1, \dotsc, \vt{\mu}_K\} & \sim & \prod\limits_{k=1}^K\left(\text{Bern}(\vt{x}_n|\vt{\mu}_k)\right)^{z_{nk}} \nonumber
\end{eqnarray}
Derive the EM algorithm for maximizing the posterior probability $p(\vt{\mu},\vt{\pi}|\{ \vt{x}_n\}_{n=1}^N)$. (The E step is given in Bishop's Book, you only need to do the M step)

\begin{sol}
	The M step consists in the maximization of the expectation of the complete
	log-likelihood. In the case of the model in Figure \ref{fig:mixbern} it is
	given by
	\begin{equation}
		\begin{split}
			\Ex_{\vt{Z}}[\ln p(\vt{X},\vt{Z},\vt{\mu},\vt{\pi})]
			&=\ln\frac{\Gamma(\sum_{k=1}^K\alpha_k)}{\prod_{k=1}^K\Gamma(\alpha_k)}
			+\sum_{k=1}^K\left[
				D\ln\frac{\Gamma(a_k+b_k)}{\Gamma(a_k)\Gamma(b_k)}
				+(\alpha_k-1)\ln\pi_k
			\right. \\
			&\left.
				+(a_k-1)\sum_{i=1}^D\ln\mu_{ki}
				+(b_k-1)\sum_{i=1}^D(1-x_{ni})\ln(1-\mu_{ki})
			\right] \\
			&+ \sum_{n=1}^N\sum_{k=1}^K \gamma(z_{nk}) \left[
				\ln\pi_k
				+\sum_{i=1}^Dx_{ni}\ln\mu_{ki}
				+\sum_{i=1}^D(1-x_{ni})(1-\mu_{ki})
			\right]
		\end{split}
	\end{equation}
	In the M step we need to maximize the expectation of the complete data
	log-likelihood w.r.t. the parameters $\vt{\mu}_k$ and $\vt{\pi}$. To do this
	we evaluate the partial derivatives w.r.t. $\mu_{ki}$
	\begin{align}
		\frac{\partial}{\partial\mu_{ki}}\Ex_{\vt{Z}}[\ln p(\vt{X},\vt{Z},\vt{\mu},\vt{\pi})]
		&=\frac{a_k-1}{\mu_{ki}}-\frac{b_k-1}{1-\mu_{ki}}
		+\sum_{n=1}^N\sum_{k=1}^K \left\{
			\gamma(z_{nk})
			\right. \nonumber \\ & \left.
			\times \frac{\partial}{\partial\mu_{ki}}
			\left[
				\ln\pi_k
				+\sum_{i=1}^Dx_{ni}\ln\mu_{ki}
				+\sum_{i=1}^D(1-x_{ni})\ln(1-\mu_{ki})
			\right]
		\right\} \\
		&=\frac{a_k-1}{\mu_{ki}}-\frac{b_k-1}{1-\mu_{ki}}
		+\sum_{n=1}^N \gamma(z_{nk}) \left[
						\frac{x_{ni}}{\mu_{ki}}
						-\frac{1-x_{ni}}{1-\mu_{ki}}
		\right] \\
		&=\frac{a_k-1}{\mu_{ki}}-\frac{b_k-1}{1-\mu_{ki}}
		+\frac{\sum_{n=1}^N\gamma(z_{nk})x_{ni}}{\mu_{ki}}
		-\frac{\sum_{n=1}^N(1-x_{ni})\gamma(z_{nk})}{1-\mu_{ki}} \\
		&=\frac{1}{\mu_{ki}(1-\mu_{ki})} \left[
		(1-\mu_{ki})(a_k-1)-\mu_{ki}(b_k-1)+(1-\mu_{ki})
		\sum_{n=1}^N\gamma(z_{nk})x_{ni}
		\right. \nonumber \\ & \left.
		-\mu_{ki}\sum_{n=1}^N\gamma(z_{nk})(1-x_{ni}) \right] \\
		&=\frac{1}{\mu_{ki}(1-\mu_{ki})} \left[
			a_k - 1 - \mu_{ki}a_k + \mu_{ki} - \mu_{ki}b_k + \mu_{ki}
			+ \sum_{n=1}^N \gamma(z_{nk})x_{ni}
			\right. \nonumber \\ & \left.
			-\mu_{ki} \sum_{n=1}^N \gamma(z_{nk}) x_{ni}
			-\mu_{ki} \sum_{n=1}^N \gamma(z_{nk})
			+\mu_{ki} \sum_{n=1}^N \gamma(z_{nk})x_{ni}
		\right]
	\end{align}
	and by setting this derivative to zero and solving for $\mu_{ki}$
	\begin{align}
			0&=a_k - 1 - \mu_{ki}a_k + \mu_{ki} - \mu_{ki}b_k + \mu_{ki}
				+ \sum_{n=1}^N \gamma(z_{nk})x_{ni}
				- \mu_{ki} \sum_{n=1}^N \gamma(z_{nk}) \\
				1 - a_k - \sum_{n=1}^N \gamma(z_{nk}) x_{ni}
				&=
				\mu_{ki}\left[ 2 - a_k - b_k - \sum_{n=1}^N \gamma(z_{nk}) \right] \\
				\mu_{ki}&=
				\frac{1 - a_k - \sum_{n=1}^N \gamma(z_{nk})x_{ni}}{2-a_k-b_k-\sum_{n=1}^N\gamma(z_{nk})}
				=\frac{\sum_{n=1}^N\gamma(z_{nk})x_{ni}+a_k-1}{\sum_{n=1}^N\gamma(z_{nk})+a_k+b_k-2}
	\end{align}
	which rewritten in vector form gives
	\begin{align}
		\vt{\mu}_k&=
		\frac{\sum_{n=1}^N\gamma(z_{nk})\vt{x}_n+a_k-1}{\sum_{n=1}^N\gamma(z_{nk})+a_k+b_k-2}
	\end{align}

	Next we need to maximize the expectation of the log-likelihood w.r.t. $\pi_k$
	using Lagrange multipliers to enforce the constraint $\sum_{k=1}^K\pi_k=1$
	\begin{align}
		\frac{\partial}{\partial\pi_k}\Ex_{\vt{Z}}[\ln p(\vt{X},\vt{Z},\vt{\mu},\vt{\pi})]
		&=\frac{\alpha_k-1}{\pi_k} + \frac{\sum_{n=1}^N \gamma(z_{nk})}{\pi_k} + \lambda \\
		&=\frac{\alpha_k-1+\sum_{n=1}^N \gamma(z_{nk})+\lambda\pi_k}{\pi_k}
	\end{align}
	and by setting this derivative to zero and solving for $\pi_k$
	\begin{align}
		0&=\alpha_k-1+\sum_{n=1}^N \gamma(z_{nk})+\lambda\pi_k \\
		\pi_k &= \frac{1-\alpha_k-\sum_{n=1}^N \gamma(z_{nk})}{\lambda}
	\end{align}
	Using the constraint on $\vt{\pi}$ we can find the value of $\lambda$
	\begin{align}
		\sum_{k=1}^K \pi_k &= 1 \\
		\frac{1}{\lambda} \sum_{k=1}^K \left[ 1-\alpha_k-\sum_{n=1}^N \gamma(z_{nk}) \right]
		&=1 \\
		\lambda&=K-\sum_{k=1}^K\alpha_k-\sum_{k=1}^K\sum_{n=1}^N \gamma(z_{nk}) \\
		\lambda&=K-\sum_{k=1}^K \alpha_k - N
	\end{align}
	substituting back we get
	\begin{align}
		\pi_k&=\frac{1-\alpha_k-\sum_{n=1}^N \gamma(z_{nk})}{K-\sum_{k=1}^K\alpha_k -N}
	\end{align}
\end{sol}

\end{problem}

\begin{problem}\textsf{Bishop 10.38}
Verify the results of the calculation of mean, variance of $q^{\backslash n}(\vt{\theta})$ and normalizing constant $Z_n$ for the expectation propagation algorithm applied to the cluster problem
\begin{eqnarray}
\vt{m}^{\backslash n} & = & \vt{m} + v^{\backslash n}v_n^{-1}(\vt{m}-\vt{m}_n)\\
(v^{\backslash n})^{-1} & = & v^{-1} - v_n^{-1}\\
Z_n & = & (1-w)\distNorm\left(\vt{x}_n|\vt{m}^{\backslash n}, (v^{\backslash n}+1)\vt{I}\right) + w\distNorm\left(\vt{x}_n|\vt{0},a\vt{I}\right)
\end{eqnarray}

Hint: see 10.38 in the book for more hint. You need equation (2.115) in Bishop to solve this homework.
\end{problem}

\end{document}
