\documentclass[a4paper,landscape]{amsmlaj}
\usepackage{anysize}
\usepackage[fontsize=9pt]{scrextend}
\usepackage{multicol}
\usepackage{mathtools}


\author{Andrea Jemmett}
\title{Machine Learning 2 - Cheat Sheet}
\date{\today}

\pagenumbering{gobble}
\marginsize{.2in}{.2in}{-.2in}{-.2in}

% Turn off header and footer
\pagestyle{empty}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}

\makeatletter
% Redefine maketitle
\def\maketitle{
	\textbf{\@title} \\
	\@author\quad-\quad\small{\@date}
}
% Redefine sections
\renewcommand{\section}{\@startsection{section}{1}{0mm}
	{-0ex plus -.1ex minus -.5ex}
	{0.1ex plus .7ex}
	{\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}
	{-1ex plus -.5ex minus -.2ex}
	{0.1ex plus .2ex}
	{\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}
	{-1ex plus -.5ex minus -.2ex}
	{0.1ex plus .2ex}
	{\normalfont\footnotesize\bfseries}}
\makeatother

% \tikzcircle command to create circles in text
\newcommand{\tikzcircle}[2][black]{
	\tikz[baseline=-0.5ex]\draw[#1,radius=#2](0,0) circle ;
}

\begin{document}
\maketitle
\begin{multicols*}{3}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\section{Probability Theory}
\subsection{Independence}
$p(X,Y)=p(X)p(Y) \Leftrightarrow p(X|Y)=p(X) \Leftrightarrow p(Y|X)=p(Y)$
\subsection{Conditional Independence}
$X \ci Y \mid Z \Longleftrightarrow p(X,Y|Z)=p(X|Z)p(Y|Z)$ \\
$p(X,Y|Z)=p(X|Y,Z)p(Y|Z)=p(X|Z)p(Y|Z)$
\subsection{Sum and Product Rules}
$p(X,Y)=p(X)p(Y|X)$, \qquad $p(X,Y,Z)=p(X)p(Y|X)p(Z|X,Y)$ \\
$p(X)=\sum_Y p(X,Y)$
\subsection{Bayes' Theorem}
$p(Y|X)=\frac{p(X|Y)p(Y)}{p(X)}$, \qquad
$p(Y|X,Z)=\frac{p(X|Y,Z)p(Y|Z)}{p(X|Z)}$

\subsection{Chain Rule}
$p(x_a,x_b,x_c) = p(x_c|x_a,x_b)p(x_b|x_a)p(x_a)$ \qquad
$p(\vt{x}) = \prod_{k=1}^K p(x_k|{\mathrm{pa}_k})$

\subsection{Misc}
$p(a,b|c) = \frac{p(a,b,c)}{p(c)}$ \qquad
$p(a,b,c) = p(a|c)p(b|c)p(c)$

\section{Distributions}
\begin{tabular}{l|lll}
	\textbf{Binary} & Bernoulli & Binomial & Beta\\
	\textbf{Discrete} & Categorical & Multinomial & Dirichlet
\end{tabular}

\subsection{Bernoulli Distribution}
$\mathrm{Ber}(x|n) = \mu^x(1-\mu)^{1-x}$, \qquad
$\Ex[x]=\mu$, \qquad
$\Var[x] = \mu -\mu^2$, \\
$P(D,\mu) = \prod_{n=1}^N \mu^{x_n}(1 - \mu)^{1-x_n}$, \qquad
$\mu_\text{ML} = \frac{1}{N} \sum_{n=1}^N x_n$

\subsection{Binomial Distribution}
$\mathrm{Bin}(m|N,\mu) = \binom{N}{m} \mu^m(1-\mu)^{N-m}$, \qquad
$\frac{n!}{k!(n-k)!} = \binom{n}{k}$, \\
$\Ex[m] = N\mu$, \qquad
$\Var[m] = N\mu(1-\mu)$, \qquad
$\mu_\text{ML} = \frac{m}{N}$

\subsection{Categorical Distribution}
$p(\vt{x}|\vt{\mu}) = \prod_{k} \mu_k^{x_k}$, \qquad
$\vt{\mu} \in \{0,1\}^K$, \qquad
$\sum_k \mu_k = 1$, \qquad
$\vt{\mu}_\text{ML} = \frac{\vt{m}}{N}$, \\
$m_k = \sum_n x_{nk}$, \qquad
$\mathrm{Mult}(m_1 \ldots, m_k|N, \vt{\mu}) = (\frac{N!}{m_1!,\ldots,m_k} \prod_{k}) \mu_k^{mk}$
\subsection{Beta Distribution}
$\mathrm{Beta}(\mu|a,b) = \frac{\Gamma(a + b)}{\Gamma(a)\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1}$,
\footnote{
	$\Gamma(x) = \int_0^1 u^{x-1}e^{-u} = 1$, \qquad
	$\Gamma(x+1) = \Gamma(x)x$, \qquad
	$\Gamma(x+1) = x!$
}\qquad
$\Ex[\mu] = \frac{a}{a + b}$, \\
$\Var[x] = \frac{ab}{(a+b)^2(a+b+1)}$, \qquad
$p(\mu|m,l,a,b) \propto \mu^{m+a-1}(1-\mu)^{l+b-1}$

\subsection{Gamma Distribution}
$\mathrm{Gamma}(\tau | a,b) = \frac{b^a}{\Gamma(a)}\tau^{a-1}e^{-b\tau}$, \qquad
$\Ex[\tau] = \frac{a}{b}$, \qquad
$\Var[\tau] = \frac{a}{b^2}$, \\
$\mathrm{mode}[\tau] = \frac{a-1}{b}$ for $a \ge 1$, \qquad
$\Ex[\ln \tau] = \psi(a) - \ln b$, \\
$H(\tau) = ln \Gamma(a) - (a-1)\psi(a) - \ln b + a$

\subsection{Multinomial Distribution}
$\vt{x} = [0,0,0,0,1,0,0]^\trans$, \qquad
$\sum_{k=1}^K x_k = 1$, \qquad
$p(\vt{x}|\vt{\mu}) = \prod_{k=1}^K \mu_k^{x_k}$, \\
$\sum_{k=1}^K \mu_k = 1$, \qquad
$\mu_k^\text{ML} = \frac{m_k}{N}$, \qquad
$m_k = \sum_{k=1}^K x_{nk}$

\subsection{Dirichlet Distribution}
$\mathrm{Dir}(\vt{\mu}|\vt{\alpha}) =
	\frac{\Gamma(\alpha_0)}{\Gamma(\alpha_1) \cdots \Gamma(\alpha_k)}
	\prod_{k=1}^K \mu_k^{\alpha_k-1}$, \qquad
$\alpha_0 = \sum_{k=1}^K \alpha_k$

\subsection{Gaussian Distribution}
$\distNorm(x|\mu,\sigma) = \frac{1}{\sqrt{2 \pi\sigma^2}}\exp(-\frac{1}{2\sigma^2}(x-\mu)^2)$,\\
$\distNorm(\vt{x}|\vt{\mu},\vt{\Sigma}) = (2\pi)^{-\frac{D}{2}}|\vt{\Sigma}|^{-\frac{1}{2}}
	\exp\left\{-\frac{1}{2}(\vt{x}-\vt{\mu})^\trans\vt{\Sigma}^{-1}(\vt{x}-\vt{\mu})\right\}$

\subsubsection{ML for the Gaussian}
$\ln p(X|\vt{\mu},\vt{\Sigma}) = -\frac{ND}{2}\ln(2\pi) - \frac{N}{2}\ln|\vt{\Sigma}|
	-\frac{1}{2}\sum_{n=1}^N (\vt{x}_n - \vt{\mu})^\trans\vt{\Sigma}^{-1}(\vt{x}_n - \vt{\mu})$,\\
$\vt{\mu}_\text{ML} = 1/N \sum_{n=1}^N \vt{x}_n$, \qquad
$\vt{\Sigma}_\text{ML} = 1/N \sum_{n=1}^N (\vt{x}_n - \vt{\mu})^\trans (\vt{x}_n - \vt{\mu})$

\subsubsection{Stochastic gradient descent Gaussian}
$\max\ P(x_1,\cdots,x_n|\theta)$,
$\theta^N = \theta^{N-1} + \alpha_{N-1} \frac{\partial}{\partial\theta^{N-1}} \ln\ p(x_n|\theta^{N-1})$

\subsubsection{Marginal and Conditional Gaussians}
Given $p(\vt{x}) = \distNorm(\vt{x}|\vt{\mu},\vt{\Lambda}^{-1})$ and
$p(\vt{y}|\vt{x}) = \distNorm(\vt{y}|\vt{Ax}+\vt{b}, \vt{L}^{-1})$. We get \\
$p(\vt{y}) =
\distNorm(\vt{y}|\vt{A\mu}+\vt{b},\vt{L}^{-1}+\vt{A}\vt{\Lambda}^{-1}\vt{A}^\trans)$ and \\
$p(\vt{x}|\vt{y}) =
\distNorm(\vt{x}|\vt{\Sigma}[\vt{A}^\trans\vt{L}(\vt{y}-\vt{b})+\vt{\Lambda}\vt{\mu}],\vt{\Sigma})$ \\
where $\vt{\Sigma} = (\vt{\Lambda}+\vt{A}^\trans\vt{L}\vt{A})^{-1}$.

\subsection{Student's T distribution}
The heavy tail of the student-t distribution makes it more
robust against outliers.\\
$St(x|\mu,\lambda,\nu) =
\frac{\Gamma(\nu/2+1/2)}{\Gamma(\nu/2)}(\frac{\lambda^{1/2}}{(\pi
\nu)^{D/2}})
[1+\frac{\lambda(x-\mu)^2}{\nu}]^{-\nu/2-D/2}$,\\
$f_{x}(x) = \frac{\Gamma[(\nu + p)/2]}{\Gamma(\nu/2)
				\nu^{p/2} \pi^{p/2}|\Sigma|^{1/2}[1 +
				1/\nu(x-\mu)^T
\Sigma^{-1}(x-\mu)]^{(\nu+p)/2}}$\\
$\mathbb{E}(\mathbf{x}) =
\frac{\Gamma(D/2+v/2)}{\Gamma(v/2)}
\frac{|\Lambda|^{1/2}}{(\pi v)^{D/2}} \int
[1+\frac{(x-\mu)^T \Lambda
(x-\mu)}{v}]^{-D/2-v/2}x\mathrm{d}x   $

\section{Independent Component Analysis}
We have a set of N observations:\\
$D = \lbrace \mathbf{x}^{(n)} \in \mathbb{R}^J \rbrace ^{N}_{n=1}$\\
, with $I$ independent sources. Thus:\\
$\mathbf{x} = \mathbf{G}\mathbf{s}$, where $G$ is not known. We assume that the
latent variables are independently distributed, with marginal distributions
$P(s_i|H) = p_i(s_i)$. The probability of the observables and the hidden nodes
is:\\
$P(\lbrace x^{(n)},s^{(n)} \rbrace_{n=1}^{N}|G,H) =
\prod_{n=1}^N[P(x^{(n)}|s^{(n)},G,H)P(s^{(n)}|H)]$\\
$= \prod_{n=1}^N[(\prod_J \delta(x_j^{(n)} - \sum_i G_{ji}s_i^{(n)}))(\prod_i
p_i(s_i^{(n)}))]$.\\ The factor x is generated without noise. For learning the G
from the data the likelihood is: $P(D|G,H) = \prod_n P(x^{(n)}|G,H)$\\
, which is a product of factors if we marginalize over the latent variables. We
can express each coefficient of $\mathbf{G}$, as a summation over all
coefficients multiplied by a delta function such that, $G_{ji}s_i^{(n)} = \sum_i
G_{ji}s_i^{(n)}$. ($\mathcal{H}$ denotes the model.)\\
$
p(\mathbf{x^{(n)}} | \mathbf{G}, \mathcal{H}) = \int p(\mathbf{x^{(n)}} |
\mathbf{s}^{(n)}, \mathbf{G}, \mathcal{H}) p(\mathbf{s}^{(n)}| \mathcal{H})
\mathrm{d}^I\mathbf{s}^{(n)}\\
= \int \prod_j \delta (x_j^{(n)} - G_{ji}s_i^{(n)}) \prod_i
p_i(s_i^{(n)})\mathrm{d}^I\mathbf{s}^{(n)}\\
= \frac{1}{|\det\ \mathbf{G}|} \prod_i p_i(G_{ji}^{-1}x_j) \Rightarrow \\
\ln p(\mathbf{x^{(n)}} | \mathbf{G}, H) = -\ln |\det \mathbf{G}| + \sum_i \ln
p_i(G_{ji}^{-1}x_j)
$\\
, which is the log-likelihood of the data D. Now, to find the gradient of the
log-likelihood, we are introducing $\mathbf{W} = \mathbf{G}^{-1}$, now the
log-likelihood can be written as:\\
$
\ln p(\mathbf{x^{(n)}} | \mathbf{G}, H) = -\ln |\det\mathbf{W}| + \sum_i \ln
p_i(W_{ji}x_j)
$\\
The gradient of the log-likelihood equation will be:\\
$
\frac{\partial}{\partial W_{ij}}\ln p(\mathbf{x^{(n)}} | \mathbf{G},
\mathcal{H}) = -\frac{\partial}{\partial W_{ij}}(\ln|\det \mathbf{W}|) +
\frac{\partial}{\partial W_{ij}}(\sum_i \ln p_i(W_{ji}x_j))
$

\section{Generative Models for Discrete Data}
We can classify a feature vector $\vt{x}$ using the Bayes rule \\
$p(y=c|\vt{x},\vt{\theta})\propto p(\vt{x}|y=c,\vt{\theta})p(y=c|\vt{\theta})$ \\
We can use different models for the data when it's discrete, based on what kind
of distribution we expect the data to assume and a respective \textit{conjugate}
prior over the model parameters $\vt{\theta}$.

\subsection{Beta-Binomial Model}
In this model we can observe a series of Bernoulli trials (e.g. coin tosses) or
the number of heads (and the number of tails or total number of tosses), which
is a Binomial, and it would result in the same \textbf{likelihood}:
$p(\mathcal{D}|\theta)=\theta^{N_1}(1-\theta)^{N_0}$. A conjugate \textbf{prior} for this
likelihood is given by $\text{Beta}(\theta|a,b)\propto\theta^{a-1}(1-\theta)^{b-1}$.
The \textbf{posterior} is then obtained by multiplying the prior with the likelihood,
$p(\theta|\mathcal{D})\propto p(\mathcal{D}|\theta)p(\theta) =
\text{Bin}(N_1|\theta,N_1+N_0)\text{Beta}(\theta|a,b)\propto\text{Beta}(\theta|a+N_1,b+N_0)$.
The \textbf{evidence} is obtained from
$p(\theta|\mathcal{D})=\frac{1}{p(\mathcal{D})}p(\mathcal{D}|\theta)p(\theta)$
normalization of posterior is $1/B(a+N_1,b+N_0)$\footnote{
				$B(a,b)=\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$
}, of prior is $1/B(a,b)$, hence
$p(\mathcal{D})=\binom{N}{N_1}B(a+N_1,b+N_0)/B(a,b)$.

\subsection{Dirichlet-Multinomial Model}
For example $N$ dice rolls or \textit{multinomial} events (with $K$ outcomes).
The \textbf{likelihood} is
$p(\mathcal{D}|\vt{\theta})=\prod_{k=1}^K\theta_k^{N_k}$ where $N_k$ counts
times event $k$ occurred (it's a suff. statistic). A conjugate \textbf{prior} is
the Dirichlet distribution. The \textbf{posterior} is
$p(\vt{\theta}|\mathcal{D})\propto\text{Dir}(\vt{\theta}|\vt{\alpha})p(\mathcal{D}|\vt{\theta})
\propto\text{Dir}(\vt{\theta}|\alpha_1+N_1,\ldots,\alpha_k+N_k)$. The
\textbf{evidence} is (obtained as the previous model)
$p(\mathcal{D})=B(\vt{N}+\vt{\alpha})/B(\vt{\alpha})$.

\section{Graphical Models}
Capture noise, for reasoning, enormous datasets, for causality, for designing
models, CIR are encoded in the graph, for inference.

\subsection{Discriminative}
$p(\vt{t}|w) = p(w) \prod_{n=1}^N p(t_n|w)$

\subsection{Generative}
$T^* = P(T^*|W,X^*,X_i,T_i,\sigma^2,a)=$,
$[\prod_{i=1}^N P(X_i|W,\sigma^2)]$
$P(W|a)P(T^*|X^*,W,\sigma^2)$

\subsection{Directed GMs a.k.a. Bayesian Networks}
Nodes are connected by \textit{directed} arrows. The full joint distribution is\\
$p(\vt{x})=\prod_{k=1}^K p(x_k|\text{pa}(x_k))$ \\
\textbf{Directed Acyclic Graphs} are BNs without \textit{directed} loops.

\subsubsection{Blocking Rules}
$\displaystyle \tikzcircle{3pt} \rightarrow \tikzcircle[fill=gray]{3pt}\rightarrow \tikzcircle{3pt}\qquad
\displaystyle \tikzcircle{3pt} \leftarrow \tikzcircle[fill=gray]{3pt} \rightarrow \tikzcircle{3pt}\qquad
\displaystyle \tikzcircle{3pt} \rightarrow \cdots \tikzcircle{3pt} \cdots \leftarrow \tikzcircle{3pt}$

\subsubsection{D-separation}
$A \ci B \mid C$ holds if each path that connects a node in $A$ with a node in
$B$ is \textit{blocked}, that is \\
\textbf{a)} the arrows on the path meet either head-to-tail or tail-to-tail
at the node and the node is in $C$, \textit{or} \\
\textbf{b)} the arrows meed head-to-head at the node and either the
node nor any of its descendants is in $C$.

\subsection{Markov Random Fields}
Graphical Models with undirected edges (a.k.a. Undirected Graphical Models).

\subsubsection{Conditional Independence}
$A \ci B \mid C$ holds if all paths connecting every node in $A$ to every other
node in $B$ is 'blocked' by a node in $C$ (blocked means passing through).

\subsubsection{Cliques and Maximal Cliques}
A \textit{clique} is a subset of nodes in the graph such that there exists a link between
all pairs of nodes in the subset (the set of nodes in a clique is fully connected).
A \textit{maximal clique} is a clique such that it is not possible to include
any other nodes from the graph in without it ceasing to be a clique.

\subsubsection{Factorization}
A MRF can be factorized using \textit{potential functions} over its maximal
cliques: $p(\vt{x})=\frac{1}{Z}\prod_C \psi_C(\vt{x}_C)$ \quad
$Z=\sum_x \prod_C \psi_C(\vt{x}_C)$

\subsubsection{Relation to Directed Graphs}
To transform a directed graph into an undirected one we have to perform a
\textit{moralization} process of ``marrying the parents'' of each node
(by linking them) and removing all remaining arrows.

\subsection{Markov Blanket}
$p(x_i|x_{\text{MB}_i},x_\text{rest})=p(x_i|x_{\text{MB}_i})$ \\
\textbf{Directed case}: The MB of $x_i$ consists of: the parents of $x_i$, the
children of $x_i$ and the co-parents of the children of $x_i$.\\
\textbf{Undirected case}: All neighboring nodes of $x_i$.

\subsection{Naive Bayes}
The problem of finding a label $y^*$ for some previously unobserved vector of
features $\vt{x}^*$, while having previously observed $\{\vt{x}_n,y_n\}_{n=1,\ldots,N}$.
We build a \textit{generative} GM, linking the label $y_n$ with each feature of
the feature vector $\vt{x}_n$. This implies that all features are independent of
each other given the label. For a single data case the joint probability is \\
$p(y,x_1,\ldots,x_D|\vt{\eta},\vt{\theta}_i)=p(y|\vt{\eta})\prod_{i=1}^D p(x_i|\vt{\theta}_i)$ \\
while the probability of the full dataset is
$\prod_{n=1}^N p(y_n|\vt{\eta})\prod_{i=1}^D p(x_i|\vt{\theta}_i)$ \\
Note that this generative form is chosen as the other option (features pointing
towards the label) would imply a very highly parameterized model, as we would
considering $p(y|x_1,\ldots,x_D)$. \\
Finding a label $y^*$ implies finding \\
$y^*=\argmax_y\left\{ \ln p(y|\vt{\eta})+\sum_{i=1}^D \ln p(x_i^*|y,\vt{\theta}_i) \right\}$

\subsection{Maximum Likelihood Training in BNs}
It is fast because the log-likelihood \textit{decomposes} into a sum over all
variables $X_i$. Learning all parameters reduces into a collection of
independent tasks of learning $p(x_i|\text{pa}_{x_i})$. \\
$p(x_i|\text{pa}_{x_i})=\frac{N(x_i,\text{pa}_{x_i})}{N(\text{pa}_{x_i})}$ \\
is the number of times $x_i$ co-occurred with $\text{pa}_{x_i}$ divided by the
number of times $\text{pa}_{x_i}$ occurred.

The probability of a set of variables $\mathbf{x}$ in a Bayes Net is defined as
$p(x) = \prod_i p(x_i|x_{pa_i})$. If we define a function $\theta(x_i,
x_{pa_i})$ and use it instead of $p(x_i |x_{pa_i})$, we have $p(x) = \prod_i
\theta(x_i, x_{pa_i})$, if we constrain $\theta$ such that $\sum_{x_i}
\theta(x_i , x_{pa_i}) = 1, \forall i$.
We now have that the probability of the full data set:
$p(\{\widetilde x_{in}\}) = \prod_n \prod_i \prod_{x_i}
\prod_{x_{pa_{i}}} \theta(x_i, x_{pa_i})
^{\mathbb{I}[x_i = ̃\widetilde x_{in} \land x_{pa_i} = ̃\widetilde x_{pa_in}]}$

\subsection{Inference in Graphical Models}
In which some of the variables are observed and we wish to compute the posterior
distribution of one or more subsets of other variables.

\subsubsection{Inference on a chain}
$p(\vt{x})=p(x_1,\ldots,x_N)$\\
$ = \frac{1}{Z}\psi_{1,2}(x_1,x_2)\ldots\psi_{N-1,N}(x_{N-1},x_N)$\\
$ = \sum_{x_1}\sum_{x_2}\ldots\sum_{x_{N-1}}\sum_{x_N}p(\vt{x})$\\
$ = \sum_{x_1} \ldots \sum_{x_{n-1}} \sum_{x_{n+1}}\ldots \sum_{x_{N-1}} \sum_{x_{N}} p(\vt{x})$\\
$ = \frac{1}{Z}\sum_{x_1}\ldots\sum_{x_{n-1}}\psi_{x_1,x_2}\ldots\psi_{x_{n-1},x_n} \mu_\beta(x_n)$\\
$\mu_\beta(x_n) = \frac{1}{Z} \sum_{x_{n+1}}\psi_{x_{n},x_{n+1}} \ldots \sum_{x_N}\psi_{x_{N-1},x_{N}}$\\
$\mu_\alpha(x_n) = \frac{1}{Z} \sum_{x_{n-1}}\psi_{x_{n-1},x_{n}} \ldots \sum_{x_1}\psi_{x_{2},x_{1}}$\\
$p(x_n) = \frac{1}{z} \mu_\alpha(x_n) \mu_\beta(x_n)$ \qquad $O(NK^2)$

\subsubsection{Factor Graphs}
A tree is a graph with no loops. Both directed and undirected trees can be
converted to a factor graph tree, but a directed tree could result in a non-tree
structure when converted to an undirected representation. It is called a
poly-tree (and not simply a tree) since its undirected representation (middle
graph) includes a loop. The factor graph representation is again a tree. Factor
graphs are the most general representation, and since any other tree
representation can be easily converted to a factor tree, the sum-product
algorithm is defined for factor trees.

\subsubsection{Sum-Product Algorithm}
It is used to find \textit{individual} marginals. \\
Probability of the factor graph: $p(\vt{x}) = \frac{1}{Z}\prod_\alpha f_\alpha(\vt{x}_\alpha)$\\
\textbf{factor $\rightarrow$ variable} message:\\
$\displaystyle \mu_{f_\alpha\rightarrow x_i}(x_i) =
\sum_{\vt{x}_\alpha \smallsetminus x_i} f_\alpha (\vt{x}_\alpha)
\prod_{m \in \text{ne}(f_\alpha) \smallsetminus x_i}\mu_{x_m\rightarrow f_\alpha}(x_m)$\\
\textbf{variable $\rightarrow$ factor} message: \\
$\displaystyle \mu_{x_m\rightarrow f_\alpha}(x_m) =
\prod_{\beta \in \text{ne}(x_m)\smallsetminus f_\alpha} \mu_{f_\beta\rightarrow x_m}(x_m)$\\
\begin{tabular}{@{}llll}\\
	\textbf{leaf nodes}& $x_l$ & is variable: & $\mu_{x_l \rightarrow f_\delta}(x_l) = 1$\\
	&$f_\varepsilon$ & is factor: & $\mu_{f_\varepsilon\rightarrow x_k}(x_k) = f_\varepsilon(x_k)$
\end{tabular} \\
\textbf{variable marginal}:
$\displaystyle p(x)=\prod_{\alpha \in \text{ne}(x)} \mu_{f_\alpha \rightarrow x}(x)$ \\
\textbf{factor marginal}:
$\displaystyle p(\vt{x}_\alpha) = f_\alpha(\vt{x}_\alpha)
\prod_{i \in \text{ne}(f_\alpha)} \mu_{x_i \rightarrow f_\alpha}(x_i)$ \\
By caching intermediate results and calculating two messages for each edge in
a tree-structured factor graph, the Sum-Product algorithm efficiently computes
all variable marginals in $O(2EK^2)$ (where $E$ is the number of edges and each
variable is assumed to have $K$ possible values).

\subsubsection{Max-Sum Algorithm}
It is used to find a setting of the variables that has the largest
\textit{joint} probability and the value of that probability. \\
$\displaystyle\vt{x}^\text{max} = \argmax_{\vt{x}} \prod_\alpha f_\alpha(\vt{x}_\alpha)\qquad
p(\vt{x}^\text{max}) = \max_{\vt{x}} \frac{1}{Z} \prod_\alpha f_\alpha(\vt{x}_\alpha)\\
\log p(\vt{x}^\text{max}) = \max_{\vt{x}}\left\{\log Z + \sum_\alpha f_\alpha(\vt{x}_\alpha)\right\}\\$
\textbf{factor $\rightarrow$ variable} message:\\
$\displaystyle \mu_{f_\alpha \rightarrow x_i}(x_i) = \max_{\vt{x}_\alpha
\setminus x_i} \left[ (\ln f_\alpha(\vt{x}_\alpha)) +
\sum_{m \in \text{ne}(f_\alpha) \setminus x_i} \mu_{x_m \rightarrow f_\alpha}(x_m) \right] $\\
\textbf{variable $\rightarrow$ factor} message:\\
$\displaystyle \mu_{x_m \rightarrow f_\alpha}(x_m) =
\sum_{f_\beta \in \mathrm{ne}(x_m) \setminus f_\alpha} \mu_{f_\beta \rightarrow x_m}(x_m) \\$
\begin{tabular}{@{}llll}\\
	\textbf{leaf nodes} & $x_l$ & is variable: & $\mu_{x_l \rightarrow f_\delta} = 0$ \\
	& $f_\varepsilon$ & is factor: & $\mu_{f_\varepsilon \rightarrow x_k} = \ln f_\varepsilon(x_k)$
\end{tabular} \\
\begin{tabular}{@{}ll}
	\textbf{at root} & $p^\text{max}=
		\max_x \left[ \sum_{f_\alpha \in \text{ne}(x)}\mu_{f_\alpha \rightarrow x}(x) \right]$
	\\
	&$x^\text{max}=
		\argmax_x \left[ \sum_{f_\alpha \in \text{ne}(x)}\mu_{f_\alpha \rightarrow x}(x) \right]$ \\
	\textbf{max-marginals} & $q(x_i)=
	-\ln Z + \sum_{f_\alpha \in \text{ne}(x_i)} \mu_{f_\alpha \rightarrow x_i}(x_i)$
\end{tabular} \\
Given max-marginals you have to perform a decoding step (the Viterbi algorithm)
in order to find the global optimum. If $q(x_i)$ has unique maximum, we can use
$x_i^* = \argmax_{x_i} q(x_i)$.

\subsubsection{Variable Elimination}
If we have some clusters $C_i$ and $C_j$, then:\\
$ S_{ij} = C_i \cap C_j $
Define potential functions $\Psi_i$ for each cluster $C_i$:
$ \forall C_i: \Psi_i \coloneqq \prod_{\psi_j \in C_i} \psi_j$
$ m_i \rightarrow C_j = \sum_{C_i \smallsetminus  S_{ij}}
\Psi_i\prod_{k \in \mathrm{ne}(i) \smallsetminus j} m_{k \rightarrow i}(C_i)$\\
Note that: $ C_i \setminus S_{ij} = C_i \setminus C_j $\\
Belief: $ \Psi_{\text{root}} \prod_{k \in \mathrm{ne}(\text{root})} m_{k
\rightarrow \text{root}}(C_{\text{root}}) \propto
p(C_{\text{root}})$\\
Clusters replace factors, edges replace variables.\\
\textbf{Tree width} The width of a tree is one smaller than the
biggest clique in the optimal ordering. Finding the optimal
order is NP-hard.\\
\textbf{Running Intersection Property} A clique tree has the
running intersection property if $x \in C_i \land x \in C_j \Rightarrow x \in C_k$
on the clique path between $C_i$ and $C_j$.

\subsubsection{Loopy Belief Propagation}
Is a way to approximate inference in graphs with loops. The idea is simply to
apply the sum-product algorithm even though there is no guarantee that it will
yield good results. This approach is possible because the message passing rules
are purely local. However, because of the loops, information can flow many times
around the graph; for some models it will converge, for others it will not. We
will say that a node $a$ has a \textbf{pending} message on its link to a node
$b$ if node $a$ has received any message on any of its other links since the
last time it sent a message to $b$. Thus, when a node receives a message on one
of its links, this creates pending messages on all of its other links. Then we
need to send only pending messages otherwise they will be duplicate. When there
are no more pending messages, the product of the received messages at every node
gives the (exact for tree) marginal. For loopy graphs however the algorithm may
not converge and when we stop the marginals are \textit{approximate}.

\section{Mathematical Tricks}
$-\frac{1}{2}(x-\mu)^\top\Sigma^{-1}(x-\mu) = -\frac{1}{2}x^\top \Sigma^{-1}x +
x^\top \Sigma^{-1} \mu + c$\\
$\frac{\partial a^\top X b}{\partial X} = ab^\top$,
$\frac{\partial a^\top X^\top b}{\partial X} = ba^\top$\\
$\frac{\partial \det X}{\partial X} = \det (X) (X^{-1})^\top$ \\
$M = M^\top \iff u^\top M v = v^\top M u$\\
$\mathbb{E}[(x-\mu)^\top \Sigma^{-1} (x-\mu)\mathcal{N}(x|\mu, \Sigma)] =
\mathrm{Tr}(\Sigma^{-1}\Sigma) = D$\\
$\mathbb{E} [x] = \int^\infty_{-\infty}x f(x)\mathrm{d}x$\\
Product of Gaussians: $\Sigma = (\Sigma_1^{-1} + \Sigma_2^{-1})^{-1}$, $\mu =
\Sigma \Sigma_1^{-1} \mu_1 + \Sigma \Sigma_2^{-1} \mu_2$\\
Other: $(P^{-1}+B^\top R^{-1}B)^{-1}B^\top R^{-1} = PB^\top (BPB^\top +
R)^{-1}$\\
Woodbury: $(A+BD^{-1}C)^{-1} = A^{-1} - A^{-1}B(D+CA^{-1}B)^{-1}CA^{-1}$\\
Gradient in exp family wrt for $\eta$, for $p(x|\eta) =
h(x)g(\eta)\text{exp}\{\eta^Tu(x)\}$, where $g(\eta)\int h(x)
\text{exp}\{\eta^Tu(x)\} dx = 1$, we have the result:$- \nabla \text{ln }
g(\eta) = \mathbb{E}[u(x)]$


\end{multicols*}
\end{document}

