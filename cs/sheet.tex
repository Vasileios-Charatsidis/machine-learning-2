\documentclass[a4paper,landscape]{amsmlaj}
\usepackage{anysize}
\usepackage[fontsize=9pt]{scrextend}
\usepackage{multicol}


\author{Andrea Jemmett}
\title{Machine Learning 2 - Cheat Sheet}
\date{\today}

\pagenumbering{gobble}
\marginsize{.2in}{.2in}{-.2in}{-.2in}

% Turn off header and footer
\pagestyle{empty}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}

\makeatletter
% Redefine maketitle
\def\maketitle{
	\textbf{\@title} \\
	\@author\quad-\quad\small{\@date}
}
% Redefine sections
\renewcommand{\section}{\@startsection{section}{1}{0mm}
	{-0ex plus -.1ex minus -.5ex}
	{0.1ex plus .7ex}
	{\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}
	{-1ex plus -.5ex minus -.2ex}
	{0.1ex plus .2ex}
	{\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}
	{-1ex plus -.5ex minus -.2ex}
	{0.1ex plus .2ex}
	{\normalfont\footnotesize\bfseries}}
\makeatother

% \tikzcircle command to create circles in text
\newcommand{\tikzcircle}[2][black]{
	\tikz[baseline=-0.5ex]\draw[#1,radius=#2](0,0) circle ;
}

\begin{document}
\maketitle
\begin{multicols*}{3}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\section{Probability Theory}
\subsection{Independence}
$p(X,Y)=p(X)p(Y) \Leftrightarrow p(X|Y)=p(X) \Leftrightarrow p(Y|X)=p(Y)$
\subsection{Conditional Independence}
$X \ci Y \mid Z \Longleftrightarrow p(X,Y|Z)=p(X|Z)p(Y|Z)$
\subsection{Sum and Product Rules}
$p(X,Y)=p(X)p(Y|X)$, \qquad $p(X,Y,Z)=p(X)p(Y|X)p(Z|X,Y)$ \\
$p(X)=\sum_Y p(X,Y)$
\subsection{Bayes' Theorem}
$p(Y|X)=\frac{p(X|Y)p(Y)}{p(X)}$, \qquad
$p(Y|X,Z)=\frac{p(X|Y,Z)p(Y|Z)}{p(X|Z)}$

\section{Distributions}
\begin{tabular}{l|lll}
	\textbf{Binary} & Bernoulli & Binomial & Beta\\
	\textbf{Discrete} & Categorical & Multinomial & Dirichlet
\end{tabular}

\subsection{Bernoulli Distribution}
$\mathrm{Ber}(x|n) = \mu^x(1-\mu)^{1-x}$, \qquad
$\Ex[x]=\mu$, \qquad
$\Var[x] = \mu -\mu^2$, \\
$P(D,\mu) = \prod_{n=1}^N \mu^{x_n}(1 - \mu)^{1-x_n}$, \qquad
$\mu_\text{ML} = \frac{1}{N} \sum_{n=1}^N x_n$

\subsection{Binomial Distribution}
$\mathrm{Bin}(m|N,\mu) = \binom{N}{m} \mu^m(1-\mu)^{N-m}$, \qquad
$\frac{n!}{k!(n-k)!} = \binom{n}{k}$, \\
$\Ex[m] = N\mu$, \qquad
$\Var[m] = N\mu(1-\mu)$, \qquad
$\mu_\text{ML} = \frac{m}{N}$

\subsection{Categorical Distribution}
$p(\vt{x}|\vt{\mu}) = \prod_{k} \mu_k^{x_k}$, \qquad
$\vt{\mu} \in \{0,1\}^K$, \qquad
$\sum_k \mu_k = 1$, \qquad
$\vt{\mu}_\text{ML} = \frac{\vt{m}}{N}$, \\
$m_k = \sum_n x_{nk}$, \qquad
$\mathrm{Mult}(m_1 \ldots, m_k|N, \vt{\mu}) = (\frac{N!}{m_1!,\ldots,m_k} \prod_{k}) \mu_k^{mk}$
\subsection{Beta Distribution}
$\mathrm{Beta}(\mu|a,b) = \frac{\Gamma(a + b)}{\Gamma(a)\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1}$,
\footnote{
	$\Gamma(x) = \int_0^1 u^{x-1}e^{-u} = 1$, \qquad
	$\Gamma(x+1) = \Gamma(x)x$, \qquad
	$\Gamma(x+1) = x!$
}\qquad
$\Ex[\mu] = \frac{a}{a + b}$, \\
$\Var[x] = \frac{ab}{(a+b)^2(a+b+1)}$, \qquad
$p(\mu|m,l,a,b) \propto \mu^{m+a-1}(1-\mu)^{l+b-1}$

\subsection{Gamma Distribution}
$\mathrm{Gamma}(\tau | a,b) = \frac{b^a}{\Gamma(a)}\tau^{a-1}e^{-b\tau}$, \qquad
$\Ex[\tau] = \frac{a}{b}$, \qquad
$\Var[\tau] = \frac{a}{b^2}$, \\
$\mathrm{mode}[\tau] = \frac{a-1}{b}$ for $a \ge 1$, \qquad
$\Ex[\ln \tau] = \psi(a) - \ln b$, \\
$H(\tau) = ln \Gamma(a) - (a-1)\psi(a) - \ln b + a$

\subsection{Multinomial Distribution}
$\vt{x} = [0,0,0,0,1,0,0]^\trans$, \qquad
$\sum_{k=1}^K x_k = 1$, \qquad
$p(\vt{x}|\vt{\mu}) = \prod_{k=1}^K \mu_k^{x_k}$, \\
$\sum_{k=1}^K \mu_k = 1$, \qquad
$\mu_k^\text{ML} = \frac{m_k}{N}$, \qquad
$m_k = \sum_{k=1}^K x_{nk}$

\subsection{Dirichlet Distribution}
$\mathrm{Dir}(\vt{\mu}|\vt{\alpha}) =
	\frac{\Gamma(\alpha_0)}{\Gamma(\alpha_1) \cdots \Gamma(\alpha_k)}
	\prod_{k=1}^K \mu_k^{\alpha_k-1}$, \qquad
$\alpha_0 = \sum_{k=1}^K \alpha_k$

\subsection{Gaussian Distribution}
$\distNorm(x|\mu,\sigma) = \frac{1}{\sqrt{2 \pi\sigma^2}}\exp(-\frac{1}{2\sigma^2}(x-\mu)^2)$,\\
$\distNorm(\vt{x}|\vt{\mu},\vt{\Sigma}) = (2\pi)^{-\frac{D}{2}}|\vt{\Sigma}|^{-\frac{1}{2}}
	\exp\left\{-\frac{1}{2}(\vt{x}-\vt{\mu})^\trans\vt{\Sigma}^{-1}(\vt{x}-\vt{\mu})\right\}$

\subsubsection{ML for the Gaussian}
$\ln p(X|\vt{\mu},\vt{\Sigma}) = -\frac{ND}{2}\ln(2\pi) - \frac{N}{2}\ln|\vt{\Sigma}|
	-\frac{1}{2}\sum_{n=1}^N (\vt{x}_n - \vt{\mu})^\trans\vt{\Sigma}^{-1}(\vt{x}_n - \vt{\mu})$,\\
$\vt{\mu}_\text{ML} = 1/N \sum_{n=1}^N \vt{x}_n$, \qquad
$\vt{\Sigma}_\text{ML} = 1/N \sum_{n=1}^N (\vt{x}_n - \vt{\mu})^\trans (\vt{x}_n - \vt{\mu})$

\subsubsection{Stochastic gradient descent Gaussian}
$\max\ P(x_1,\cdots,x_n|\theta)$,
$\theta^N = \theta^{N-1} + \alpha_{N-1} \frac{\partial}{\partial\theta^{N-1}} \ln\ p(x_n|\theta^{N-1})$

\subsubsection{Marginal and Conditional Gaussians}
Given $p(\vt{x}) = \distNorm(\vt{x}|\vt{\mu},\vt{\Lambda}^{-1})$ and
$p(\vt{y}|\vt{x}) = \distNorm(\vt{y}|\vt{Ax}+\vt{b}, \vt{L}^{-1})$. We get \\
$p(\vt{y}) =
\distNorm(\vt{y}|\vt{A\mu}+\vt{b},\vt{L}^{-1}+\vt{A}\vt{\Lambda}^{-1}\vt{A}^\trans)$ and \\
$p(\vt{x}|\vt{y}) =
\distNorm(\vt{x}|\vt{\Sigma}[\vt{A}^\trans\vt{L}(\vt{y}-\vt{b})+\vt{\Lambda}\vt{\mu}],\vt{\Sigma})$ \\
where $\vt{\Sigma} = (\vt{\Lambda}+\vt{A}^\trans\vt{L}\vt{A})^{-1}$.

\subsection{Student's T distribution}
The heavy tail of the student-t distribution makes it more
robust against outliers.\\
$St(x|\mu,\lambda,\nu) =
\frac{\Gamma(\nu/2+1/2)}{\Gamma(\nu/2)}(\frac{\lambda^{1/2}}{(\pi
\nu)^{D/2}})
[1+\frac{\lambda(x-\mu)^2}{\nu}]^{-\nu/2-D/2}$,\\
$f_{x}(x) = \frac{\Gamma[(\nu + p)/2]}{\Gamma(\nu/2)
				\nu^{p/2} \pi^{p/2}|\Sigma|^{1/2}[1 +
				1/\nu(x-\mu)^T
\Sigma^{-1}(x-\mu)]^{(\nu+p)/2}}$\\
$\mathbb{E}(\mathbf{x}) =
\frac{\Gamma(D/2+v/2)}{\Gamma(v/2)}
\frac{|\Lambda|^{1/2}}{(\pi v)^{D/2}} \int
[1+\frac{(x-\mu)^T \Lambda
(x-\mu)}{v}]^{-D/2-v/2}x\mathrm{d}x   $

\section{Graphical Models}
Capture noise, for reasoning, enormous datasets, for causality, for designing
models, CIR are encoded in the graph, for inference.

\subsection{Directed GMs a.k.a. Bayesian Networks}
Nodes are connected by \textit{directed} arrows. The full joint distribution is\\
$p(\vt{x})=\prod_{k=1}^K p(x_k|\text{pa}(x_k))$ \\
\textbf{Directed Acyclic Graphs} are BNs without \textit{directed} loops.

\subsubsection{Blocking Rules}
$\displaystyle \tikzcircle{3pt} \rightarrow \tikzcircle[fill=gray]{3pt}\rightarrow \tikzcircle{3pt}\qquad
\displaystyle \tikzcircle{3pt} \leftarrow \tikzcircle[fill=gray]{3pt} \rightarrow \tikzcircle{3pt}\qquad
\displaystyle \tikzcircle{3pt} \rightarrow \cdots \tikzcircle{3pt} \cdots \leftarrow \tikzcircle{3pt}$

\subsubsection{D-separation}
$A \ci B \mid C$ holds if each path that connects a node in $A$ with a node in
$B$ is \textit{blocked}, that is \\
\textbf{a)} the arrows on the path meet either head-to-tail or tail-to-tail
at the node and the node is in $C$, \textit{or} \\
\textbf{b)} the arrows meed head-to-head at the node and either the
node nor any of its descendants is in $C$.

\subsection{Markov Random Fields}
Graphical Models with undirected edges (a.k.a. Undirected Graphical Models).

\subsubsection{Conditional Independence}
$A \ci B \mid C$ holds if all paths connecting every node in $A$ to every other
node in $B$ is 'blocked' by a node in $C$ (blocked means passing through).

\subsubsection{Cliques and Maximal Cliques}
A \textit{clique} is a subset of nodes in the graph such that there exists a link between
all pairs of nodes in the subset (the set of nodes in a clique is fully connected).
A \textit{maximal clique} is a clique such that it is not possible to include
any other nodes from the graph in without it ceasing to be a clique.

\subsubsection{Factorization}
A MRF can be factorized using \textit{potential functions} over its maximal
cliques: $p(\vt{x})=\frac{1}{Z}\prod_C \psi_C(\vt{x}_C)$ \quad
$Z=\sum_x \prod_C \psi_C(\vt{x}_C)$

\subsubsection{Relation to Directed Graphs}
To transform a directed graph into an undirected one we have to perform a
\textit{moralization} process of ``marrying the parents'' of each node
(by linking them) and removing all remaining arrows.

\subsection{Markov Blanket}
$p(x_i|x_{\text{MB}_i},x_\text{rest})=p(x_i|x_{\text{MB}_i})$ \\
\textbf{Directed case}: The MB of $x_i$ consists of: the parents of $x_i$, the
children of $x_i$ and the co-parents of the children of $x_i$.\\
\textbf{Undirected case}: All neighboring nodes of $x_i$.

\subsection{Naive Bayes}
The problem of finding a label $y^*$ for some previously unobserved vector of
features $\vt{x}^*$, while having previously observed $\{\vt{x}_n,y_n\}_{n=1,\ldots,N}$.
We build a \textit{generative} GM, linking the label $y_n$ with each feature of
the feature vector $\vt{x}_n$. This implies that all features are independent of
each other given the label. For a single data case the joint probability is \\
$p(y,x_1,\ldots,x_D|\vt{\eta},\vt{\theta}_i)=p(y|\vt{\eta})\prod_{i=1}^D p(x_i|\vt{\theta}_i)$ \\
while the probability of the full dataset is
$\prod_{n=1}^N p(y_n|\vt{\eta})\prod_{i=1}^D p(x_i|\vt{\theta}_i)$ \\
Note that this generative form is chosen as the other option (features pointing
towards the label) would imply a very highly parameterized model, as we would
considering $p(y|x_1,\ldots,x_D)$. \\
Finding a label $y^*$ implies finding \\
$y^*=\argmax_y\left\{ \ln p(y|\vt{\eta})+\sum_{i=1}^D \ln p(x_i^*|y,\vt{\theta}_i) \right\}$

\subsection{Maximum Likelihood Training in BNs}
It is fast because the log-likelihood \textit{decomposes} into a sum over all
variables $X_i$. Learning all parameters reduces into a collection of
independent tasks of learning $p(x_i|\text{pa}_{x_i})$. \\
$p(x_i|\text{pa}_{x_i})=\frac{N(x_i,\text{pa}_{x_i})}{N(\text{pa}_{x_i})}$ \\
is the number of times $x_i$ co-occurred with $\text{pa}_{x_i}$ divided by the
number of times $\text{pa}_{x_i}$ occurred.

\subsection{Inference in Graphical Models}
In which some of the variables are observed and we wish to compute the posterior
distribution of one or more subsets of other variables.

\subsubsection{Inference on a chain}
$p(\vt{x})=p(x_1,\ldots,x_N)$\\
$ = \frac{1}{Z}\psi_{1,2}(x_1,x_2)\ldots\psi_{N-1,N}(x_{N-1},x_N)$\\
$ = \sum_{x_1}\sum_{x_2}\ldots\sum_{x_{N-1}}\sum_{x_N}p(\vt{x})$\\
$ = \sum_{x_1} \ldots \sum_{x_{n-1}} \sum_{x_{n+1}}\ldots \sum_{x_{N-1}} \sum_{x_{N}} p(\vt{x})$\\
$ = \frac{1}{Z}\sum_{x_1}\ldots\sum_{x_{n-1}}\psi_{x_1,x_2}\ldots\psi_{x_{n-1},x_n} \mu_\beta(x_n)$\\
$\mu_\beta(x_n) = \frac{1}{Z} \sum_{x_{n+1}}\psi_{x_{n},x_{n+1}} \ldots \sum_{x_N}\psi_{x_{N-1},x_{N}}$\\
$\mu_\alpha(x_n) = \frac{1}{Z} \sum_{x_{n-1}}\psi_{x_{n-1},x_{n}} \ldots \sum_{x_1}\psi_{x_{2},x_{1}}$\\
$p(x_n) = \frac{1}{z} \mu_\alpha(x_n) \mu_\beta(x_n)$ \qquad $O(NK^2)$

\subsubsection{Factor Graphs}
A tree is a graph with no loops. Both directed and undirected trees can be
converted to a factor graph tree, but a directed tree could result in a non-tree
structure when converted to an undirected representation. It is called a
poly-tree (and not simply a tree) since its undirected representation (middle
graph) includes a loop. The factor graph representation is again a tree. Factor
graphs are the most general representation, and since any other tree
representation can be easily converted to a factor tree, the sum-product
algorithm is defined for factor trees.

\subsubsection{Sum-product algorithm}
Probability of the factor graph: $p(\vec{x}) = \frac{1}{Z}\prod_\alpha f_\alpha(\vec{x}_\alpha)$\\
\textbf{factor $\rightarrow$ variable message}
$\displaystyle \mu_{\alpha\rightarrow i}(x_i) =
\sum_{x_{\alpha \smallsetminus i}}f_\alpha (\vec{x}_\alpha) \prod_{j\in \alpha \smallsetminus i}\mu_{j\rightarrow \alpha}$\\
\textbf{variable $\rightarrow$ factor message}
$\displaystyle \mu_{j\rightarrow \alpha}(x_j) =
\prod_{\beta \in \mathrm{ne}(j)\smallsetminus\alpha} \mu_{\beta\rightarrow j}(x_j)$\\
\begin{tabular}{@{}llll}\\
	\textbf{leaf node messages}& $x_l$ & is a leaf node: & $\mu_{l\rightarrow \delta}(x_l) = 1$\\
	&$\varepsilon$ & is a leaf node: & $\mu_{\varepsilon\rightarrow k}(x_k) = f_\varepsilon(x_k)$
\end{tabular}


\end{multicols*}
\end{document}

